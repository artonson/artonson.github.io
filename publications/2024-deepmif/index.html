<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping | Alexey Artemov</title>
<link rel=stylesheet href=https://artonson.github.io/css/style.css><link rel=stylesheet href=https://artonson.github.io/css/fonts.css></head><body><nav><ul class=menu><li><a href=https://artonson.github.io/>Home</a></li><li><a href=https://artonson.github.io/cv_alexey_artemov.pdf>CV</a></li><li><a href=https://artonson.github.io/publications>Research</a></li><li><a href=https://artonson.github.io/teaching>Education</a></li><li><a href=https://artonson.github.io/art>Art & Heritage</a></li><li><a href=https://artonson.github.io/press>Press</a></li><li class=menu-standard><a href=mailto:artonson@yandex.ru><img src=https://simpleicons.org/icons/maildotru.svg style=max-width:3%;min-width:18px alt=E-Mail></a></li><li class=menu-standard><a href=https://github.com/artonson><img src=https://simpleicons.org/icons/github.svg style=max-width:3%;min-width:18px alt="Github repo"></a></li><li class=menu-standard><a href="https://scholar.google.ru/citations?user=5lVsH-IAAAAJ&hl=en"><img src=https://simpleicons.org/icons/googlescholar.svg style=max-width:3%;min-width:18px alt="Google Scholar"></a></li><li class=menu-standard><a href=https://www.youtube.com/channel/UCO946RPg2Yp2PXFzon9_73g><img src=https://simpleicons.org/icons/youtube.svg style=max-width:3%;min-width:18px alt=Youtube></a></li><li class=menu-standard><a href=https://www.linkedin.com/artonson><img src=https://simpleicons.org/icons/linkedin.svg style=max-width:3%;min-width:18px alt=LinkedIn></a></li><li class=menu-standard><a href=https://twitter.com/artonson><img src=https://simpleicons.org/icons/twitter.svg style=max-width:3%;min-width:18px alt=Twitter></a></li><li class=menu-standard><a href=https://t.me/research_meets_art><img src=https://simpleicons.org/icons/telegram.svg style=max-width:3%;min-width:18px alt="Telegram Channel"></a></li></ul><hr></nav><div class=article-meta><h1>DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping</h1></div><div class=single-authors-wrap><table class=single-authors id=single-authors><tr><td>Kutay Yılmaz</td><td>Matthias Nießner</td><td>Anastasiia Kornilova</td><td>Alexey Artemov</td></tr><tr></tr></table></div><div class=publication-single-teaser><img src=https://artonson.github.io/publications/2024-deepmif/image-big.jpg width=768/></div><div class=publication-single-venue>ArXiv</div><div><h2>Abstract</h2><span class=publications-single-abstract>Recently, significant progress has been achieved in sensing real large-scale outdoor 3D environments, particularly by using modern acquisition equipment such as LiDAR sensors. Unfortunately, they are fundamentally limited in their ability to produce dense, complete 3D scenes. To address this issue, recent learning-based methods integrate neural implicit representations and optimizable feature grids to approximate surfaces of 3D scenes. However, naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results due to the nature of sparse, conflicting LiDAR measurements. Instead, in this work we depart from fitting LiDAR data exactly, instead letting the network optimize a non-metric monotonic implicit field defined in 3D space. To fit our field, we design a learning system integrating a monotonicity loss that enables optimizing neural monotonic fields and leverages recent progress in large- scale 3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as captured by multiple quantitative and perceptual measures and visual results obtained for Mai City, Newer College, and KITTI benchmarks. The code of our approach will be made publicly available.</span></div><div><h2>Video</h2><iframe class=video width=100% height=400 src=https://www.youtube.com/embed/WC-pcaf94N4 frameborder=0 allow="autoplay; encrypted-media" allowfullscreen></iframe></div><main><h2 id=sample-reconstruction-results>Sample Reconstruction Results</h2><p>We have tested our approach vs. <a href=https://github.com/JunyuanDeng/NeRF-LOAM>NeRF-LOAM</a>,
<a href=https://github.com/PRBonn/make_it_dense>Make it Dense</a>,
<a href=https://github.com/PRBonn/SHINE_mapping>SHINE-Mapping</a>,
<a href=https://github.com/PRBonn/puma>PUMA</a>,
and <a href=https://github.com/PRBonn/vdbfusion>VDBFusion</a>.</p><h3 id=mai-city>Mai City</h3><p>Mai City is a simulated environment commonly used for benchmarking scene reconstruction algorithms.</p><table class=video-gallery><thead><tr><th></th><th></th></tr></thead><tbody><tr><td><span class=video-player-container><span id=mai_city_shaded class=video-player></span>
</span><script type=text/javascript src=https://cdn.jsdelivr.net/npm/@clappr/player@latest/dist/clappr.min.js></script><script>var playerElement=document.getElementById("mai_city_shaded"),player=new Clappr.Player({source:"mai_city_shaded.mp4",mute:"true",autoPlay:"true",loop:"true",height:"225",width:"400",disableKeyboardShortcuts:!0,hideVolumeBar:!0,playback:{hlsjsConfig:{maxBufferLength:5}}});player.attachTo(playerElement)</script></td><td><span class=video-player-container><span id=newer_college_color class=video-player></span>
</span><script type=text/javascript src=https://cdn.jsdelivr.net/npm/@clappr/player@latest/dist/clappr.min.js></script><script>var playerElement=document.getElementById("newer_college_color"),player=new Clappr.Player({source:"newer_college_color.mp4",mute:"true",autoPlay:"true",loop:"true",height:"225",width:"400",disableKeyboardShortcuts:!0,hideVolumeBar:!0,playback:{hlsjsConfig:{maxBufferLength:5}}});player.attachTo(playerElement)</script></td></tr></tbody></table><h3 id=newer-college>Newer College</h3><p>Newer College is a building belonging to Oxford University. It was scanned using a hand-held LiDAR (common input data for reconstruction) and a terrestrial industrial LiDAR (common reference data for evaluation).</p><h3 id=kitti>KITTI</h3><p>KITTI is the best known, and the most commonly used benchmark for autonomous driving algorithms.
The data used was a Velodyne LiDAR scanned mounted on a Volkswagen.</p></main><div><h2>Resources</h2><ul><li>Source code: <a href=https://github.com/artonson/deepmif>https://github.com/artonson/deepmif</a></li><li>Video <a href=https://youtu.be/WC-pcaf94N4>Video</a></li></ul></div><footer><hr>© <a href=https://artonson.github.io>Alexey Artemov</a> 2021–2024 | <a href=mailto:artonson@yandex.ru>Email</a> | <a href=https://github.com/artonson>Github</a> | <a href="https://scholar.google.ru/citations?user=5lVsH-IAAAAJ&amp;hl=en">Scholar</a> | <a href=https://www.youtube.com/channel/UCO946RPg2Yp2PXFzon9_73g>YouTube</a> | <a href=https://www.linkedin.com/in/artonson/>LinkedIn</a> | <a href=https://twitter.com/artonson>Twitter</a> | <a href=https://t.me/research_meets_art>Telegram</a></footer></body></html>