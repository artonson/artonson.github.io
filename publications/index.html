<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Publications | Alexey Artemov</title>
<link rel=stylesheet href=https://artonson.github.io/css/style.css><link rel=stylesheet href=https://artonson.github.io/css/fonts.css></head><body><nav><ul class=menu><li><a href=https://artonson.github.io/>Home</a></li><li><a href=https://artonson.github.io/cv_alexey_artemov.pdf>CV</a></li><li><a href=https://artonson.github.io/publications>Research</a></li><li><a href=https://artonson.github.io/teaching>Education</a></li><li><a href=https://artonson.github.io/art>Art & Heritage</a></li><li class=menu-standard><a href=mailto:artonson@yandex.ru><img src=https://simpleicons.org/icons/maildotru.svg style=max-width:3%;min-width:18px alt=E-Mail></a></li><li class=menu-standard><a href=https://github.com/artonson><img src=https://simpleicons.org/icons/github.svg style=max-width:3%;min-width:18px alt="Github repo"></a></li><li class=menu-standard><a href="https://scholar.google.ru/citations?user=5lVsH-IAAAAJ&hl=en"><img src=https://simpleicons.org/icons/googlescholar.svg style=max-width:3%;min-width:18px alt="Google Scholar"></a></li><li class=menu-standard><a href=https://www.youtube.com/channel/UCO946RPg2Yp2PXFzon9_73g><img src=https://simpleicons.org/icons/youtube.svg style=max-width:3%;min-width:18px alt=Youtube></a></li><li class=menu-standard><a href=https://www.linkedin.com/artonson><img src=https://simpleicons.org/icons/linkedin.svg style=max-width:3%;min-width:18px alt=LinkedIn></a></li><li class=menu-standard><a href=https://twitter.com/artonson><img src=https://simpleicons.org/icons/twitter.svg style=max-width:3%;min-width:18px alt=Twitter></a></li><li class=menu-standard><a href=https://t.me/research_meets_art><img src=https://simpleicons.org/icons/telegram.svg style=max-width:3%;min-width:18px alt="Telegram Channel"></a></li></ul><hr></nav><h1>Publications</h1><p>This page contains a list of recent scientific publications and preprints,
sorted from oldest to newest.
You might be interested in other webpages that keep track of my publications,
such as <a href="https://scholar.google.com/citations?user=5lVsH-IAAAAJ">Google Scholar page</a>,
<a href=http://arxiv.org/a/artemov_a_2>Arxiv page</a>, or <a href="https://www.scopus.com/authid/detail.uri?authorId=57117507600">Scopus page</a>.</p><table class=tg><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2024-deepmif/><img src=https://artonson.github.io/publications/2024-deepmif/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2024-deepmif/>DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping</a></span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Kutay Yılmaz, Matthias Nießner, Anastasiia Kornilova, <strong>Alexey Artemov</strong></span></td></tr><tr><td class=tg-0pky><span class=publications-venue>arXived 2024</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>Recently, significant progress has been achieved in sensing real large-scale outdoor 3D environments, particularly by using modern acquisition equipment such as LiDAR sensors. Unfortunately, they are fundamentally limited in their ability to produce dense, complete 3D scenes. To address this issue, recent learning-based methods integrate neural implicit representations and optimizable feature grids to approximate surfaces of 3D scenes. However, naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results due to the nature of sparse, conflicting LiDAR measurements. Instead, in this work we depart from fitting LiDAR data exactly, instead letting the network optimize a non-metric monotonic implicit field defined in 3D space. To fit our field, we design a learning system integrating a monotonicity loss that enables optimizing neural monotonic fields and leverages recent progress in large- scale 3D mapping. Our algorithm achieves high-quality dense 3D mapping performance as captured by multiple quantitative and perceptual measures and visual results obtained for Mai City, Newer College, and KITTI benchmarks. The code of our approach will be made publicly available.</span></td></tr><tr><td class=tg-links><span class=publications-links><a href=https://github.com/artonson/deepmif>Code</a>
</span><span class=publications-links><a href=https://youtu.be/WC-pcaf94N4>Video</a></span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2024-autoinst/><img src=https://artonson.github.io/publications/2024-autoinst/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2024-autoinst/>AutoInst: Automatic Instance-Based Segmentation of LiDAR 3D Scans</a></span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Cedric Perauer, Laurenz Heidrich, Haifan Zhang, Matthias Nießner, Anastasiia Kornilova, <strong>Alexey Artemov</strong></span></td></tr><tr><td class=tg-0pky><span class=publications-venue>arXived 2024</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>Recently, progress in acquisition equipment such as LiDAR sensors has enabled sensing increasingly spacious outdoor 3D environments. Making sense of such 3D acquisitions requires fine-grained scene understanding, such as constructing instance-based 3D scene segmentations. Commonly, a neural network is trained for this task; however, this requires access to a large, densely annotated dataset, which is widely known to be challenging to obtain. To address this issue, in this work we propose to predict instance segmentations for 3D scenes in an unsupervised way, without relying on ground-truth annotations. To this end, we construct a learning framework consisting of two components: (1) a pseudo-annotation scheme for generating initial unsupervised pseudo-labels; and (2) a self-training algorithm for instance segmentation to fit robust, accurate instances from initial noisy proposals. To enable generating 3D instance mask proposals, we construct a weighted proxy-graph by connecting 3D points with edges integrating multi-modal image- and point-based self-supervised features, and perform graph-cuts to isolate individual pseudo-instances. We then build on a state-of-the-art point-based architecture and train a 3D instance segmentation model, resulting in significant refinement of initial proposals. To scale to arbitrary complexity 3D scenes, we design our algorithm to operate on local 3D point chunks and construct a merging step to generate scene- level instance segmentations. Experiments on the challenging SemanticKITTI benchmark demonstrate the potential of our approach, where it attains 13.3% higher Average Precision and 9.1% higher F1 score compared to the best-performing baseline. The code will be made publicly available.</span></td></tr><tr><td class=tg-links><span class=publications-links><a href=https://arxiv.org/pdf/2403.16318.pdf>Paper</a>
</span><span class=publications-links><a href=https://github.com/artonson/autoinst>Code</a>
</span><span class=publications-links><a href="https://www.youtube.com/watch?v=ioKJWY8L6xk">Video</a></span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2024-prs/><img src=https://artonson.github.io/publications/2024-prs/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2024-prs/>PRS: Sharp Feature Priors for Resolution-Free Surface Remeshing</a></span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Natalia Soboleva, Olga Gorbunova, Maria Ivanova, Evgeny Burnaev, Matthias Nießner, Denis Zorin, <strong>Alexey Artemov</strong></span></td></tr><tr><td class=tg-0pky><span class=publications-venue>Accepted to CVPR 2024 (first arXived 2023)</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>Surface reconstruction with preservation of geometric features is a challenging computer vision task. Despite significant progress in implicit shape reconstruction, state-of-the-art mesh extraction methods often produce aliased, perceptually distorted surfaces and lack scalability to high-resolution 3D shapes. We present a data-driven approach for automatic feature detection and remeshing that requires only a coarse, aliased mesh as input and scales to arbitrary resolution reconstructions. We define and learn a collection of surface-based fields to (1) capture sharp geometric features in the shape with an implicit vertexwise model and (2) approximate improvements in normals alignment obtained by applying edge-flips with an edgewise model. To support scaling to arbitrary complexity shapes, we learn our fields using local triangulated patches, fusing estimates on complete surface meshes. Our feature remeshing algorithm integrates the learned fields as sharp feature priors and optimizes vertex placement and mesh connectivity for maximum expected surface improvement. On a challenging collection of high-resolution shape reconstructions in the ABC dataset, our algorithm improves over state-of-the-art by 26% normals F-score and 42% perceptual RMSEv.</span></td></tr><tr><td class=tg-links><span class=publications-links><a href=https://arxiv.org/pdf/2311.18494.pdf>Paper</a>
</span><span class=publications-links><a href=https://artonson.github.io/publications/2024-prs/cite.bib>Bibtex</a>
</span><span class=publications-links><a href=https://github.com/artonson/prs>Code</a>
</span><span class=publications-links><a href="https://www.youtube.com/watch?v=tm2NqXfcP4I">Video</a></span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2024-meshgpt/><img src=https://artonson.github.io/publications/2024-meshgpt/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2024-meshgpt/>MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers</a></span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Yawar Siddiqui, Antonio Alliegro, <strong>Alexey Artemov</strong>, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, Matthias Nießner</span></td></tr><tr><td class=tg-0pky><span class=publications-venue>Accepted to CVPR 2024 (first arXived 2023)</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>We introduce MeshGPT, a new approach for generating triangle meshes that reflects the compactness typical of artist-created meshes, in contrast to dense triangle meshes extracted by iso-surfacing methods from neural fields. Inspired by recent advances in powerful large language models, we adopt a sequence-based approach to autoregressively generate triangle meshes as sequences of triangles. We first learn a vocabulary of latent quantized embeddings, using graph convolutions, which inform these embeddings of the local mesh geometry and topology. These embeddings are sequenced and decoded into triangles by a decoder, ensuring that they can effectively reconstruct the mesh. A transformer is then trained on this learned vocabulary to predict the index of the next embedding given previous embeddings. Once trained, our model can be autoregressively sampled to generate new triangle meshes, directly generating compact meshes with sharp edges, more closely imitating the efficient triangulation patterns of human-crafted meshes. MeshGPT demonstrates a notable improvement over state of the art mesh generation methods, with a 9% increase in shape coverage and a 30-point enhancement in FID scores across various categories.</span></td></tr><tr><td class=tg-links><span class=publications-links><a href=https://arxiv.org/pdf/2311.15475>Paper</a>
</span><span class=publications-links><a href=https://artonson.github.io/publications/2024-meshgpt/cite.bib>Bibtex</a>
</span><span class=publications-links><a href=https://github.com/nihalsid/mesh-gpt>Code</a>
</span><span class=publications-links><a href="https://www.youtube.com/watch?v=UV90O1_69_o">Video</a>
</span><span class=publications-links><a href=https://nihalsid.github.io/mesh-gpt/>Project page</a></span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2024-ssr-2d/><img src=https://artonson.github.io/publications/2024-ssr-2d/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2024-ssr-2d/>SSR-2D: Semantic 3D Scene Reconstruction from 2D Images</a></span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Junwen Huang, <strong>Alexey Artemov</strong>, Yujin Chen, Shuaifeng Zhi, Kai Xu, Matthias Nießner</span></td></tr><tr><td class=tg-0pky><span class=publications-venue>arXived 2023</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>Most deep learning approaches to comprehensive semantic modeling of 3D indoor spaces require costly dense annotations in the 3D domain. In this work, we explore a central 3D scene modeling task, namely, semantic scene reconstruction without using any 3D annotations. The key idea of our approach is to design a trainable model that employs both incomplete 3D reconstructions and their corresponding source RGB-D images, fusing cross-domain features into volumetric embeddings to predict complete 3D geometry, color, and semantics with only 2D labeling which can be either manual or machine-generated. Our key technical innovation is to leverage differentiable rendering of color and semantics to bridge 2D observations and unknown 3D space, using the observed RGB images and 2D semantics as supervision, respectively. We additionally develop a learning pipeline and corresponding method to enable learning from imperfect predicted 2D labels, which could be additionally acquired by synthesizing in an augmented set of virtual training views complementing the original real captures, enabling more efficient self-supervision loop for semantics. In this work, we propose an end-to-end trainable solution jointly addressing geometry completion, colorization, and semantic mapping from limited RGB-D images, without relying on any 3D ground-truth information. Our method achieves state-of-the-art performance of semantic scene reconstruction on two large-scale benchmark datasets MatterPort3D and ScanNet, surpasses baselines even with costly 3D annotations. To our knowledge, our method is also the first 2D-driven method addressing completion and semantic segmentation of real-world 3D scans.</span></td></tr><tr><td class=tg-links><span class=publications-links><a href=https://arxiv.org/pdf/2302.03640>Paper</a>
</span><span class=publications-links><a href=https://artonson.github.io/publications/2024-ssr-2d/cite.bib>Bibtex</a></span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2023-sk3d/><img src=https://artonson.github.io/publications/2023-sk3d/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2023-sk3d/>Skoltech3D: Multi-sensor large-scale dataset for multi-view 3D reconstruction</a></span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Oleg Voynov, Gleb Bobrovskikh, Pavel Karpyshev, Saveliy Galochkin, Andrei-Timotei Ardelean, Arseniy Bozhenko, Ekaterina Karmanova, Pavel Kopanev, Yaroslav Labutin-Rymsho, Ruslan Rakhimov, Aleksandr Safin, Valerii Serpiva, <strong>Alexey Artemov</strong>, Evgeny Burnaev, Dzmitry Tsetserukou, Denis Zorin</span></td></tr><tr><td class=tg-0pky><span class=publications-venue>CVPR 2023 (first arXived 2022)</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>We present a multi-sensor dataset for multi-view 3D surface reconstruction. It includes registered RGB and depth data from 7 sensors of different resolutions and modalities (a): smartphones, Intel RealSense, Microsoft Kinect, industrial cameras, and structured-light scanner. The scenes are selected to emphasize a diverse set of material properties challenging for existing algorithms (c), such as featureless (F), highly specular with sharp reflections (S), or translucent (T), as illustrated with reconstructions produced by state-of-the-art algorithms (compare with an “easy” object on the bottom right). We provide around 1.4 million images of 107 different scenes acquired from 100 viewing directions under 14 lighting conditions (b). We expect our dataset will be useful for evaluation and training of 3D reconstruction algorithms and for related tasks.</span></td></tr><tr><td class=tg-links><span class=publications-links><a href=http://openaccess.thecvf.com/content/CVPR2023/papers/Voynov_Multi-Sensor_Large-Scale_Dataset_for_Multi-View_3D_Reconstruction_CVPR_2023_paper.pdf>Paper</a>
</span><span class=publications-links><a href=https://openaccess.thecvf.com/content/CVPR2023/supplemental/Voynov_Multi-Sensor_Large-Scale_Dataset_CVPR_2023_supplemental.pdf>Supplementary</a>
</span><span class=publications-links><a href=https://artonson.github.io/publications/2019-abc-dataset/cite.bib>Bibtex</a>
</span><span class=publications-links><a href=https://github.com/Skoltech-3D/sk3d_data>Code</a>
</span><span class=publications-links><a href="https://www.youtube.com/watch?v=KPwghPyZWDE">Video</a>
</span><span class=publications-links><a href=https://skoltech3d.appliedai.tech>Project page</a></span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2021-unpaired-depth-sr/><img src=https://artonson.github.io/publications/2021-unpaired-depth-sr/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2021-unpaired-depth-sr/>Towards Unpaired Depth Enhancement and Super-Resolution in the Wild</a></span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Aleksandr Safin, Maxim Kan, Nikita Drobyshev, Oleg Voynov, <strong>Alexey Artemov</strong>, Alexander Filippov, Denis Zorin, Evgeny Burnaev</span></td></tr><tr><td class=tg-0pky><span class=publications-venue>arXived 2021</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>Depth maps captured with commodity sensors are often of low quality and resolution; these maps need to be enhanced to be used in many applications. State-of-the-art data-driven methods of depth map super-resolution rely on registered pairs of low- and high-resolution depth maps of the same scenes. Acquisition of real-world paired data requires specialized setups. Another alternative, generating low-resolution maps from high-resolution maps by subsampling, adding noise and other artificial degradation methods, does not fully capture the characteristics of real-world low-resolution images. As a consequence, supervised learning methods trained on such artificial paired data may not perform well on real-world low-resolution inputs. We consider an approach to depth super-resolution based on learning from <em>unpaired data</em>. While many techniques for unpaired image-to-image translation have been proposed, most fail to deliver effective hole-filling or reconstruct accurate surfaces using depth maps. We propose an unpaired learning method for depth super-resolution, which is based on a learnable degradation model, enhancement component and surface normal estimates as features to produce more accurate depth maps. We propose a benchmark for unpaired depth SR and demonstrate that our method outperforms existing unpaired methods and performs on par with paired.</span></td></tr><tr><td class=tg-links><span class=publications-links><a href=https://arxiv.org/pdf/2105.12038>Paper</a>
</span><span class=publications-links><a href=https://artonson.github.io/publications/2021-unpaired-depth-sr/cite.bib>Bibtex</a></span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2021-towards-part-based/><img src=https://artonson.github.io/publications/2021-towards-part-based/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2021-towards-part-based/>Towards Part-Based Understanding of RGB-D Scans</a></span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Alexey Bokhovkin, Vladislav Ishimtsev, Emil Bogomolov, Denis Zorin, <strong>Alexey Artemov</strong>, Evgeny Burnaev, Angela Dai</span></td></tr><tr><td class=tg-0pky><span class=publications-venue>CVPR 2021 (first arXived 2020)</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>Recent advances in 3D semantic scene understanding have shown impressive progress in 3D instance segmentation, enabling object-level reasoning about 3D scenes; however, a finer-grained understanding is required to enable interactions with objects and their functional understanding. Thus, we propose the task of part-based scene understanding of real-world 3D environments: from an RGB-D scan of a scene, we detect objects, and for each object predict its decomposition into geometric part masks, which composed together form the complete geometry of the observed object. We leverage an intermediary part graph representation to enable robust completion as well as building of part priors, which we use to construct the final part mask predictions. Our experiments demonstrate that guiding part understanding through part graph to part prior-based predictions significantly outperforms alternative approaches to the task of semantic part completion.</span></td></tr><tr><td class=tg-links><span class=publications-links><a href=https://openaccess.thecvf.com/content/CVPR2021/papers/Bokhovkin_Towards_Part-Based_Understanding_of_RGB-D_Scans_CVPR_2021_paper.pdf>Paper</a>
</span><span class=publications-links><a href=https://openaccess.thecvf.com/content/CVPR2021/supplemental/Bokhovkin_Towards_Part-Based_Understanding_CVPR_2021_supplemental.pdf>Supplementary</a>
</span><span class=publications-links><a href=https://artonson.github.io/publications/2021-towards-part-based/cite.bib>Bibtex</a>
</span><span class=publications-links><a href=https://github.com/alexeybokhovkin/part-based-scan-understanding>Code</a>
</span><span class=publications-links><a href="https://www.youtube.com/watch?v=iuixmPNs4v4">Video</a>
</span><span class=publications-links><a href="https://www.dropbox.com/s/f4ne7k6uim7vqck/2021-towards-part-based.pptx.zip?dl=0">Slides</a>
</span><span class=publications-links><a href=https://www.3dunderstanding.org/papers/2021/bokhovkin2021towards/>Project page</a></span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2022-def/><img src=https://artonson.github.io/publications/2022-def/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2022-def/>DEF: Deep Estimation of Sharp Geometric Features in 3D Shapes</a></span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Albert Matveev, Ruslan Rakhimov, <strong>Alexey Artemov</strong>, Gleb Bobrovskikh, Vage Egiazarian, Emil Bogomolov, Daniele Panozzo, Denis Zorin, Evgeny Burnaev</span></td></tr><tr><td class=tg-0pky><span class=publications-venue>ACM Transaction on Graphics (SIGGRAPH) 2022 (first arXived 2020)</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>We propose Deep Estimators of Features (DEFs), a learning-based framework for predicting sharp geometric features in sampled 3D shapes. Differently from existing data-driven methods, which reduce this problem to feature classification, we propose to regress a scalar field representing the distance from point samples to the closest feature line on local patches. Our approach is the first that scales to massive point clouds by fusing distance-to-feature estimates obtained on individual patches.We extensively evaluate our approach against related state-of-the-art methods on newly proposed synthetic and real-world 3D CAD model benchmarks. Our approach not only outperforms these (with improvements in Recall and False Positives Rates), but generalizes to real-world scans after training our model on synthetic data and fine-tuning it on a small dataset of scanned data.We demonstrate a downstream application, where we reconstruct an explicit representation of straight and curved sharp feature lines from range scan data.We make code, pre-trained models, and our training and evaluation datasets available at <a href=https://github.com/artonson/def>https://github.com/artonson/def</a>.</span></td></tr><tr><td class=tg-links><span class=publications-links><a href=https://dl.acm.org/doi/pdf/10.1145/3528223.3530140>Paper</a>
</span><span class=publications-links><a href=https://www.dropbox.com/s/weqxi9nzknpxbra/2022-def-supp.pdf>Supplementary</a>
</span><span class=publications-links><a href=https://artonson.github.io/publications/2022-def/cite.bib>Bibtex</a>
</span><span class=publications-links><a href=https://github.com/artonson/def>Code</a>
</span><span class=publications-links><a href="https://www.youtube.com/watch?v=roiqmlBXn_k">Video</a>
</span><span class=publications-links><a href="https://www.dropbox.com/scl/fi/74ayo4dduxhzfsar1z87m/2022-def.key.zip?rlkey=bi54mtkkzilgbr6gpj2z0osuf&amp;dl=0">Slides</a></span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2020-cad-deform/><img src=https://artonson.github.io/publications/2020-cad-deform/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2020-cad-deform/>CAD-Deform: Deformable Fitting of CAD Models to 3D Scans</a></span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Vladislav Ishimtsev, Alexey Bokhovkin, <strong>Alexey Artemov</strong>, Savva Ignatyev, Matthias Nießner, Denis Zorin, Evgeny Burnaev</span></td></tr><tr><td class=tg-0pky><span class=publications-venue>ECCV 2020 (first arXived 2020)</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>Shape retrieval and alignment are a promising avenue towards turning 3D scans into lightweight CAD representations that can be used for content creation such as mobile or AR/VR gaming scenarios. Unfortunately, CAD model retrieval is limited by the availability of models in standard 3D shape collections (e.g., ShapeNet). In this work, we address this shortcoming by introducing CAD-Deform, a method which obtains more accurate CAD-to-scan fits by non-rigidly deforming retrieved CAD models. Our key contribution is a new non-rigid deformation model incorporating smooth transformations and preservation of sharp features, that simultaneously achieves very tight fits from CAD models to the 3D scan and maintains the clean, high-quality surface properties of hand-modeled CAD objects. A series of thorough experiments demonstrate that our method achieves significantly tighter scan-to-CAD fits, allowing a more accurate digital replica of the scanned real-world environment while preserving important geometric features present in synthetic CAD environments.</span></td></tr><tr><td class=tg-links><span class=publications-links><a href=https://arxiv.org/pdf/2007.11965.pdf>Paper</a>
</span><span class=publications-links><a href=https://artonson.github.io/publications/2024-ssr-2d/cite.bib>Bibtex</a>
</span><span class=publications-links><a href=https://github.com/alexeybokhovkin/CAD-Deform>Code</a>
</span><span class=publications-links><a href="https://www.youtube.com/watch?v=tzFMIzkTyVo">Video</a>
</span><span class=publications-links><a href="https://www.dropbox.com/scl/fi/ztjy8pfdit7wbsw5inhzn/2020-cad-deform.pptx.zip?rlkey=lhbnko1cpqncbw5ebp37960t9&amp;dl=0">Slides</a></span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2021-dense-pose/><img src=https://artonson.github.io/publications/2021-dense-pose/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2021-dense-pose/>Making DensePose Fast and Light</a></span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Ruslan Rakhimov, Emil Bogomolov, Alexandr Notchenko, Fung Mao, <strong>Alexey Artemov</strong>, Denis Zorin, Evgeny Burnaev</span></td></tr><tr><td class=tg-0pky><span class=publications-venue>WACV 2021 (first arXived 2020)</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>DensePose estimation task is a significant step forward for enhancing user experience computer vision applications ranging from augmented reality to cloth fitting. Existing neural network models capable of solving this task are heavily parameterized and a long way from being transferred to an embedded or mobile device. To enable Dense Pose inference on the end device with current models, one needs to support an expensive server-side infrastructure and have a stable internet connection. To make things worse, mobile and embedded devices do not always have a powerful GPU inside. In this work, we target the problem of redesigning the DensePose R-CNN model&rsquo;s architecture so that the final network retains most of its accuracy but becomes more light-weight and fast. To achieve that, we tested and incorporated many deep learning innovations from recent years, specifically performing an ablation study on 23 efficient backbone architectures, multiple two-stage detection pipeline modifications, and custom model quantization methods. As a result, we achieved 17 times model size reduction and 2 times latency improvement compared to the baseline model.</span></td></tr><tr><td class=tg-links><span class=publications-links><a href=https://openaccess.thecvf.com/content/WACV2021/papers/Rakhimov_Making_DensePose_Fast_and_Light_WACV_2021_paper.pdf>Paper</a>
</span><span class=publications-links><a href=https://artonson.github.io/publications/2021-dense-pose/cite.bib>Bibtex</a>
</span><span class=publications-links><a href=https://github.com/zetyquickly/DensePoseFnL>Code</a>
</span><span class=publications-links><a href="https://www.dropbox.com/s/89jx45uibpqjx8p/2021-dense-pose.key.zip?dl=0">Slides</a></span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2021-lvt/><img src=https://artonson.github.io/publications/2021-lvt/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2021-lvt/>Latent Video Transformer</a></span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Ruslan Rakhimov, Denis Volkhonskiy, <strong>Alexey Artemov</strong>, Denis Zorin, Evgeny Burnaev</span></td></tr><tr><td class=tg-0pky><span class=publications-venue>VISAPP 2021 (first arXived 2020)</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>The video generation task can be formulated as a prediction of future video frames given some past frames. Recent generative models for videos face the problem of high computational requirements. Some models require up to 512 Tensor Processing Units for parallel training. In this work, we address this problem via modeling the dynamics in a latent space. After the transformation of frames into the latent space, our model predicts latent representation for the next frames in an autoregressive manner. We demonstrate the performance of our approach on BAIR Robot Pushing and Kinetics-600 datasets. The approach tends to reduce requirements to 8 Graphical Processing Units for training the models while maintaining comparable generation quality.</span></td></tr><tr><td class=tg-links><span class=publications-links><a href=https://arxiv.org/pdf/2006.10704>Paper</a>
</span><span class=publications-links><a href=https://artonson.github.io/publications/2021-lvt/cite.bib>Bibtex</a>
</span><span class=publications-links><a href=https://github.com/rakhimovv/lvt>Code</a>
</span><span class=publications-links><a href="https://www.dropbox.com/scl/fi/eyo6uqep2et3p0not521c/2021-lvt.pptx.zip?rlkey=4gty4ci9x8nnz2e6plx390xvp&amp;dl=0">Slides</a></span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2020-deep-vectorization/><img src=https://artonson.github.io/publications/2020-deep-vectorization/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2020-deep-vectorization/>Deep Vectorization of Technical Drawings</a></span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Vage Egiazarian, Oleg Voynov, <strong>Alexey Artemov</strong>, Denis Volkhonskiy, Aleksandr Safin, Maria Taktasheva, Denis Zorin, Evgeny Burnaev</span></td></tr><tr><td class=tg-0pky><span class=publications-venue>ECCV 2020 (first arXived 2020)</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>We present a new method for vectorization of technical line drawings, such as floor plans, architectural drawings, and 2D CAD images. Our method includes (1) a deep learning-based cleaning stage to eliminate the background and imperfections in the image and fill in missing parts, (2) a transformer-based network to estimate vector primitives, and (3) optimization procedure to obtain the final primitive configurations. We train the networks on synthetic data, renderings of vector line drawings, and manually vectorized scans of line drawings. Our method quantitatively and qualitatively outperforms a number of existing techniques on a collection of representative technical drawings.</span></td></tr><tr><td class=tg-links><span class=publications-links><a href=https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580579.pdf>Paper</a>
</span><span class=publications-links><a href=https://static-content.springer.com/esm/chp%3A10.1007%2F978-3-030-58601-0_35/MediaObjects/504454_1_En_35_MOESM1_ESM.pdf>Supplementary</a>
</span><span class=publications-links><a href=https://artonson.github.io/publications/2020-deep-vectorization/cite.bib>Bibtex</a>
</span><span class=publications-links><a href=https://github.com/Vahe1994/Deep-Vectorization-of-Technical-Drawings>Code</a>
</span><span class=publications-links><a href="https://www.youtube.com/watch?v=lnQNzHJOLvE">Video</a>
</span><span class=publications-links><a href="https://www.dropbox.com/s/ux1lzxw6qbcqi7d/2020-deep-vectorization.pptx.zip?dl=0">Slides</a></span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2019-monocular-3d/><img src=https://artonson.github.io/publications/2019-monocular-3d/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2019-monocular-3d/>Monocular 3D Object Detection via Geometric Reasoning on Keypoints</a></span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Ivan Barabanau, <strong>Alexey Artemov</strong>, Evgeny Burnaev, Vyacheslav Murashkin</span></td></tr><tr><td class=tg-0pky><span class=publications-venue>VISAPP 2020 (first arXived 2019)</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>Monocular 3D object detection is well-known to be a challenging vision task due to the loss of depth information; attempts to recover depth using separate image-only approaches lead to unstable and noisy depth estimates, harming 3D detections. In this paper, we propose a novel keypoint-based approach for 3D object detection and localization from a single RGB image. We build our multi-branch model around 2D keypoint detection in images and complement it with a conceptually simple geometric reasoning method. Our network performs in an end-to-end manner, simultaneously and interdependently estimating 2D characteristics, such as 2D bounding boxes, keypoints, and orientation, along with full 3D pose in the scene. We fuse the outputs of distinct branches, applying a reprojection consistency loss during training. The experimental evaluation on the challenging KITTI dataset benchmark demonstrates that our network achieves state-of-the-art results among other monocular 3D detectors.</span></td></tr><tr><td class=tg-links><span class=publications-links><a href=https://arxiv.org/pdf/1905.05618>Paper</a>
</span><span class=publications-links><a href=https://artonson.github.io/publications/2019-monocular-3d/cite.bib>Bibtex</a>
</span><span class=publications-links><a href="https://www.dropbox.com/s/yyt3y9uogxajlw0/Artemov_Visapp2020_Mono3d.pptx?dl=0">Slides</a></span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2019-perceptual-deep/><img src=https://artonson.github.io/publications/2019-perceptual-deep/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2019-perceptual-deep/>Perceptual Deep Depth Super-Resolution</a></span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Oleg Voynov, <strong>Alexey Artemov</strong>, Vage Egiazarian, Alexander Notchenko, Gleb Bobrovskikh, Evgeny Burnaev, Denis Zorin</span></td></tr><tr><td class=tg-0pky><span class=publications-venue>ICCV 2019 (first arXived 2018)</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>RGBD images, combining high-resolution color and lower-resolution depth from various types of depth sensors, are increasingly common. One can significantly improve the resolution of depth maps by taking advantage of color information; deep learning methods make combining color and depth information particularly easy. However, fusing these two sources of data may lead to a variety of artifacts. If depth maps are used to reconstruct 3D shapes, e.g., for virtual reality applications, the visual quality of upsampled images is particularly important. The main idea of our approach is to measure the quality of depth map upsampling using renderings of resulting 3D surfaces. We demonstrate that a simple visual appearance-based loss, when used with either a trained CNN or simply a deep prior, yields significantly improved 3D shapes, as measured by a number of existing perceptual metrics. We compare this approach with a number of existing optimization and learning-based techniques.</span></td></tr><tr><td class=tg-links><span class=publications-links><a href=https://openaccess.thecvf.com/content_ICCV_2019/papers/Voynov_Perceptual_Deep_Depth_Super-Resolution_ICCV_2019_paper.pdf>Paper</a>
</span><span class=publications-links><a href=https://openaccess.thecvf.com/content_ICCV_2019/supplemental/Voynov_Perceptual_Deep_Depth_ICCV_2019_supplemental.pdf>Supplementary</a>
</span><span class=publications-links><a href=https://artonson.github.io/publications/2019-perceptual-deep/cite.bib>Bibtex</a>
</span><span class=publications-links><a href=https://github.com/voyleg/perceptual-depth-sr>Code</a>
</span><span class=publications-links><a href="https://www.dropbox.com/scl/fi/ijam34a36wbvi8yep9fp7/2019-perceptual-deep.zip?rlkey=7i0iojo62o17pj0niaemfqcja&amp;dl=0">Slides</a></span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2019-abc-dataset/><img src=https://artonson.github.io/publications/2019-abc-dataset/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2019-abc-dataset/>ABC: A Big CAD Model Dataset for Geometric Deep Learning</a></span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, <strong>Alexey Artemov</strong>, Evgeny Burnaev, Marc Alexa, Denis Zorin, Daniele Panozzo</span></td></tr><tr><td class=tg-0pky><span class=publications-venue>CVPR 2019 (first arXived 2018)</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>We introduce ABC-Dataset, a collection of one million Computer-Aided Design (CAD) models for research of geometric deep learning methods and applications. Each model is a collection of explicitly parametrized curves and surfaces, providing ground truth for differential quantities, patch segmentation, geometric feature detection, and shape reconstruction. Sampling the parametric descriptions of surfaces and curves allows generating data in different formats and resolutions, enabling fair comparisons for a wide range of geometric learning algorithms. As a use case for our dataset, we perform a large-scale benchmark for estimation of surface normals, comparing existing data driven methods and evaluating their performance against both the ground truth and traditional normal estimation methods.</span></td></tr><tr><td class=tg-links><span class=publications-links><a href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Koch_ABC_A_Big_CAD_Model_Dataset_for_Geometric_Deep_Learning_CVPR_2019_paper.pdf>Paper</a>
</span><span class=publications-links><a href=https://openaccess.thecvf.com/content_CVPR_2019/supplemental/Koch_ABC_A_Big_CVPR_2019_supplemental.pdf>Supplementary</a>
</span><span class=publications-links><a href=https://artonson.github.io/publications/2019-abc-dataset/cite.bib>Bibtex</a>
</span><span class=publications-links><a href=https://deep-geometry.github.io/abc-dataset/>Project page</a></span></td></tr><tr class=publications-separator><td></tr></table><footer><hr>© <a href=https://artonson.github.io>Alexey Artemov</a> 2021–2024 | <a href=mailto:artonson@yandex.ru>Email</a> | <a href=https://github.com/artonson>Github</a> | <a href="https://scholar.google.ru/citations?user=5lVsH-IAAAAJ&amp;hl=en">Scholar</a> | <a href=https://www.youtube.com/channel/UCO946RPg2Yp2PXFzon9_73g>YouTube</a> | <a href=https://www.linkedin.com/in/artonson/>LinkedIn</a> | <a href=https://twitter.com/artonson>Twitter</a> | <a href=https://t.me/research_meets_art>Telegram</a></footer></body></html>