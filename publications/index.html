<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Publications | Alexey Artemov</title>
<link rel=stylesheet href=https://artonson.github.io/css/style.css><link rel=stylesheet href=https://artonson.github.io/css/fonts.css></head><body><nav><ul class=menu><li><a href=https://artonson.github.io/>Home</a></li><li><a href=https://artonson.github.io/cv_alexey_artemov.pdf>CV</a></li><li><a href=https://artonson.github.io/publications>Publications</a></li><li><a href=https://artonson.github.io/teaching>Teaching</a></li><li><a href=https://artonson.github.io/students>Students</a></li><li><a href=https://artonson.github.io/code_resources>Code & Resources</a></li><li><a href=https://artonson.github.io/press>Press</a></li><li class=menu-standard><a href=mailto:a.artemov@skoltech.ru><img src=https://simpleicons.org/icons/maildotru.svg style=max-width:3%;min-width:20px alt=E-Mail></a></li><li class=menu-standard><a href=https://github.com/artonson><img src=https://simpleicons.org/icons/github.svg style=max-width:3%;min-width:20px alt="Github repo"></a></li><li class=menu-standard><a href="https://scholar.google.ru/citations?user=5lVsH-IAAAAJ&hl=en"><img src=https://simpleicons.org/icons/googlescholar.svg style=max-width:3%;min-width:20px alt="Google Scholar"></a></li><li class=menu-standard><a href=https://www.youtube.com/channel/UCO946RPg2Yp2PXFzon9_73g><img src=https://simpleicons.org/icons/youtube.svg style=max-width:3%;min-width:20px alt=Youtube></a></li><li class=menu-standard><a href=https://www.linkedin.com/artonson><img src=https://simpleicons.org/icons/linkedin.svg style=max-width:3%;min-width:20px alt=LinkedIn></a></li><li class=menu-standard><a href=https://twitter.com/artonson><img src=https://simpleicons.org/icons/twitter.svg style=max-width:3%;min-width:20px alt=Twitter></a></li></ul><hr></nav><h1>Publications</h1><p>This page contains a list of recent scientific publications and preprints,
sorted from oldest to newest.
You might be interested in other webpages that keep track of my publications,
such as <a href="https://scholar.google.com/citations?user=5lVsH-IAAAAJ">Google Scholar page</a>,
<a href=http://arxiv.org/a/artemov_a_2>Arxiv page</a>, or <a href="https://www.scopus.com/authid/detail.uri?authorId=57117507600">Scopus page</a>.</p><table class=tg><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2024-prs/><img src=https://artonson.github.io/publications/2024-prs/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2024-prs/>PRS: Sharp Feature Priors for Resolution-Free Surface Remeshing</a>
</span><span class=publications-venue>CVPR
2024</span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Natalia Soboleva, Olga Gorbunova, Maria Ivanova, Evgeny Burnaev, Matthias Nießner, Denis Zorin, <strong>Alexey Artemov</strong></span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>Surface reconstruction with preservation of geometric features is a challenging computer vision task. Despite significant progress in implicit shape reconstruction, state-of-the-art mesh extraction methods often produce aliased, perceptually distorted surfaces and lack scalability to high-resolution 3D shapes. We present a data-driven approach for automatic feature detection and remeshing that requires only a coarse, aliased mesh as input and scales to arbitrary resolution reconstructions. We define and learn a collection of surface-based fields to (1) capture sharp geometric features in the shape with an implicit vertexwise model and (2) approximate improvements in normals alignment obtained by applying edge-flips with an edgewise model. To support scaling to arbitrary complexity shapes, we learn our fields using local triangulated patches, fusing estimates on complete surface meshes. Our feature remeshing algorithm integrates the learned fields as sharp feature priors and optimizes vertex placement and mesh connectivity for maximum expected surface improvement. On a challenging collection of high-resolution shape reconstructions in the ABC dataset, our algorithm improves over state-of-the-art by 26% normals F-score and 42% perceptual RMSEv.</span></td></tr><tr><td class=tg-0lax><span class=publications-links>[<a href=https://arxiv.org/pdf/2311.18494.pdf>Paper</a>]
</span><span class=publications-links>[<a href=https://artonson.github.io/publications/2024-prs/cite.bib>Bibtex</a>]
</span><span class=publications-links>[<a href=https://github.com/artonson/prs>Code</a>]
</span><span class=publications-links>[<a href="https://www.youtube.com/watch?v=tm2NqXfcP4I">Video</a>]</span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2023-sk3d/><img src=https://artonson.github.io/publications/2023-sk3d/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2023-sk3d/>Skoltech3D: Multi-sensor large-scale dataset for multi-view 3D reconstruction</a>
</span><span class=publications-venue>CVPR
2023</span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Oleg Voynov, Gleb Bobrovskikh, Pavel Karpyshev, Saveliy Galochkin, Andrei-Timotei Ardelean, Arseniy Bozhenko, Ekaterina Karmanova, Pavel Kopanev, Yaroslav Labutin-Rymsho, Ruslan Rakhimov, Aleksandr Safin, Valerii Serpiva, <strong>Alexey Artemov</strong>, Evgeny Burnaev, Dzmitry Tsetserukou, Denis Zorin</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>We present a multi-sensor dataset for multi-view 3D surface reconstruction. It includes registered RGB and depth data from 7 sensors of different resolutions and modalities (a): smartphones, Intel RealSense, Microsoft Kinect, industrial cameras, and structured-light scanner. The scenes are selected to emphasize a diverse set of material properties challenging for existing algorithms (c), such as featureless (F), highly specular with sharp reflections (S), or translucent (T), as illustrated with reconstructions produced by state-of-the-art algorithms (compare with an “easy” object on the bottom right). We provide around 1.4 million images of 107 different scenes acquired from 100 viewing directions under 14 lighting conditions (b). We expect our dataset will be useful for evaluation and training of 3D reconstruction algorithms and for related tasks.</span></td></tr><tr><td class=tg-0lax><span class=publications-links>[<a href=http://openaccess.thecvf.com/content/CVPR2023/papers/Voynov_Multi-Sensor_Large-Scale_Dataset_for_Multi-View_3D_Reconstruction_CVPR_2023_paper.pdf>Paper</a>]
</span><span class=publications-links>[<a href=https://openaccess.thecvf.com/content/CVPR2023/supplemental/Voynov_Multi-Sensor_Large-Scale_Dataset_CVPR_2023_supplemental.pdf>Supplementary</a>]
</span><span class=publications-links>[<a href=https://artonson.github.io/publications/2019-abc-dataset/cite.bib>Bibtex</a>]
</span><span class=publications-links>[<a href=https://github.com/Skoltech-3D/sk3d_data>Code</a>]
</span><span class=publications-links>[<a href="https://www.youtube.com/watch?v=KPwghPyZWDE">Video</a>]
</span><span class=publications-links>[<a href=https://skoltech3d.appliedai.tech>Project page</a>]</span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2022-def/><img src=https://artonson.github.io/publications/2022-def/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2022-def/>DEF: Deep Estimation of Sharp Geometric Features in 3D Shapes</a>
</span><span class=publications-venue><em>ACM Trans. Graph.</em>
2022</span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Albert Matveev, Ruslan Rakhimov, <strong>Alexey Artemov</strong>, Gleb Bobrovskikh, Vage Egiazarian, Emil Bogomolov, Daniele Panozzo, Denis Zorin, Evgeny Burnaev</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>We propose Deep Estimators of Features (DEFs), a learning-based framework for predicting sharp geometric features in sampled 3D shapes. Differently from existing data-driven methods, which reduce this problem to feature classification, we propose to regress a scalar field representing the distance from point samples to the closest feature line on local patches. Our approach is the first that scales to massive point clouds by fusing distance-to-feature estimates obtained on individual patches.We extensively evaluate our approach against related state-of-the-art methods on newly proposed synthetic and real-world 3D CAD model benchmarks. Our approach not only outperforms these (with improvements in Recall and False Positives Rates), but generalizes to real-world scans after training our model on synthetic data and fine-tuning it on a small dataset of scanned data.We demonstrate a downstream application, where we reconstruct an explicit representation of straight and curved sharp feature lines from range scan data.We make code, pre-trained models, and our training and evaluation datasets available at <a href=https://github.com/artonson/def>https://github.com/artonson/def</a>.</span></td></tr><tr><td class=tg-0lax><span class=publications-links>[<a href=https://dl.acm.org/doi/pdf/10.1145/3528223.3530140>Paper</a>]
</span><span class=publications-links>[<a href=https://www.dropbox.com/s/weqxi9nzknpxbra/2022-def-supp.pdf>Supplementary</a>]
</span><span class=publications-links>[<a href=https://artonson.github.io/publications/2022-def/cite.bib>Bibtex</a>]
</span><span class=publications-links>[<a href=https://github.com/artonson/def>Code</a>]
</span><span class=publications-links>[<a href="https://www.youtube.com/watch?v=roiqmlBXn_k">Video</a>]</span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2021-towards-part-based/><img src=https://artonson.github.io/publications/2021-towards-part-based/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2021-towards-part-based/>Towards Part-Based Understanding of RGB-D Scans</a>
</span><span class=publications-venue>CVPR
2021</span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Alexey Bokhovkin, Vladislav Ishimtsev, Emil Bogomolov, Denis Zorin, <strong>Alexey Artemov</strong>, Evgeny Burnaev, Angela Dai</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>Recent advances in 3D semantic scene understanding have shown impressive progress in 3D instance segmentation, enabling object-level reasoning about 3D scenes; however, a finer-grained understanding is required to enable interactions with objects and their functional understanding. Thus, we propose the task of part-based scene understanding of real-world 3D environments: from an RGB-D scan of a scene, we detect objects, and for each object predict its decomposition into geometric part masks, which composed together form the complete geometry of the observed object. We leverage an intermediary part graph representation to enable robust completion as well as building of part priors, which we use to construct the final part mask predictions. Our experiments demonstrate that guiding part understanding through part graph to part prior-based predictions significantly outperforms alternative approaches to the task of semantic part completion.</span></td></tr><tr><td class=tg-0lax><span class=publications-links>[<a href=https://openaccess.thecvf.com/content/CVPR2021/papers/Bokhovkin_Towards_Part-Based_Understanding_of_RGB-D_Scans_CVPR_2021_paper.pdf>Paper</a>]
</span><span class=publications-links>[<a href=https://openaccess.thecvf.com/content/CVPR2021/supplemental/Bokhovkin_Towards_Part-Based_Understanding_CVPR_2021_supplemental.pdf>Supplementary</a>]
</span><span class=publications-links>[<a href=https://artonson.github.io/publications/2021-towards-part-based/cite.bib>Bibtex</a>]
</span><span class=publications-links>[<a href=https://github.com/alexeybokhovkin/part-based-scan-understanding>Code</a>]
</span><span class=publications-links>[<a href="https://www.youtube.com/watch?v=iuixmPNs4v4">Video</a>]
</span><span class=publications-links>[<a href="https://www.dropbox.com/s/f4ne7k6uim7vqck/2021-towards-part-based.pptx.zip?dl=0">Slides</a>]
</span><span class=publications-links>[<a href=https://www.3dunderstanding.org/papers/2021/bokhovkin2021towards/>Project page</a>]</span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2021-dense-pose/><img src=https://artonson.github.io/publications/2021-dense-pose/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2021-dense-pose/>Making DensePose Fast and Light</a>
</span><span class=publications-venue>WACV
2021</span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Ruslan Rakhimov, Emil Bogomolov, Alexandr Notchenko, Fung Mao, <strong>Alexey Artemov</strong>, Denis Zorin, Evgeny Burnaev</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>DensePose estimation task is a significant step forward for enhancing user experience computer vision applications ranging from augmented reality to cloth fitting. Existing neural network models capable of solving this task are heavily parameterized and a long way from being transferred to an embedded or mobile device. To enable Dense Pose inference on the end device with current models, one needs to support an expensive server-side infrastructure and have a stable internet connection. To make things worse, mobile and embedded devices do not always have a powerful GPU inside. In this work, we target the problem of redesigning the DensePose R-CNN model&rsquo;s architecture so that the final network retains most of its accuracy but becomes more light-weight and fast. To achieve that, we tested and incorporated many deep learning innovations from recent years, specifically performing an ablation study on 23 efficient backbone architectures, multiple two-stage detection pipeline modifications, and custom model quantization methods. As a result, we achieved 17 times model size reduction and 2 times latency improvement compared to the baseline model.</span></td></tr><tr><td class=tg-0lax><span class=publications-links>[<a href=https://openaccess.thecvf.com/content/WACV2021/papers/Rakhimov_Making_DensePose_Fast_and_Light_WACV_2021_paper.pdf>Paper</a>]
</span><span class=publications-links>[<a href=https://artonson.github.io/publications/2021-dense-pose/cite.bib>Bibtex</a>]
</span><span class=publications-links>[<a href=https://github.com/zetyquickly/DensePoseFnL>Code</a>]
</span><span class=publications-links>[<a href="https://www.dropbox.com/s/89jx45uibpqjx8p/2021-dense-pose.key.zip?dl=0">Slides</a>]</span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2021-unpaired-depth-sr/><img src=https://artonson.github.io/publications/2021-unpaired-depth-sr/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2021-unpaired-depth-sr/>Towards Unpaired Depth Enhancement and Super-Resolution in the Wild</a>
</span><span class=publications-venue>arXiv
2021</span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Aleksandr Safin, Maxim Kan, Nikita Drobyshev, Oleg Voynov, <strong>Alexey Artemov</strong>, Alexander Filippov, Denis Zorin, Evgeny Burnaev</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>Depth maps captured with commodity sensors are often of low quality and resolution; these maps need to be enhanced to be used in many applications. State-of-the-art data-driven methods of depth map super-resolution rely on registered pairs of low- and high-resolution depth maps of the same scenes. Acquisition of real-world paired data requires specialized setups. Another alternative, generating low-resolution maps from high-resolution maps by subsampling, adding noise and other artificial degradation methods, does not fully capture the characteristics of real-world low-resolution images. As a consequence, supervised learning methods trained on such artificial paired data may not perform well on real-world low-resolution inputs. We consider an approach to depth super-resolution based on learning from \emph{unpaired data}. While many techniques for unpaired image-to-image translation have been proposed, most fail to deliver effective hole-filling or reconstruct accurate surfaces using depth maps. We propose an unpaired learning method for depth super-resolution, which is based on a learnable degradation model, enhancement component and surface normal estimates as features to produce more accurate depth maps. We propose a benchmark for unpaired depth SR and demonstrate that our method outperforms existing unpaired methods and performs on par with paired.</span></td></tr><tr><td class=tg-0lax><span class=publications-links>[<a href=https://arxiv.org/pdf/2105.12038>Paper</a>]
</span><span class=publications-links>[<a href=https://artonson.github.io/publications/2021-unpaired-depth-sr/cite.bib>Bibtex</a>]</span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2020-deep-vectorization/><img src=https://artonson.github.io/publications/2020-deep-vectorization/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2020-deep-vectorization/>Deep Vectorization of Technical Drawings</a>
</span><span class=publications-venue>ECCV
2020</span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Vage Egiazarian, Oleg Voynov, <strong>Alexey Artemov</strong>, Denis Volkhonskiy, Aleksandr Safin, Maria Taktasheva, Denis Zorin, Evgeny Burnaev</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>We present a new method for vectorization of technical line drawings, such as floor plans, architectural drawings, and 2D CAD images. Our method includes (1) a deep learning-based cleaning stage to eliminate the background and imperfections in the image and fill in missing parts, (2) a transformer-based network to estimate vector primitives, and (3) optimization procedure to obtain the final primitive configurations. We train the networks on synthetic data, renderings of vector line drawings, and manually vectorized scans of line drawings. Our method quantitatively and qualitatively outperforms a number of existing techniques on a collection of representative technical drawings.</span></td></tr><tr><td class=tg-0lax><span class=publications-links>[<a href=https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123580579.pdf>Paper</a>]
</span><span class=publications-links>[<a href=https://static-content.springer.com/esm/chp%3A10.1007%2F978-3-030-58601-0_35/MediaObjects/504454_1_En_35_MOESM1_ESM.pdf>Supplementary</a>]
</span><span class=publications-links>[<a href=https://artonson.github.io/publications/2020-deep-vectorization/cite.bib>Bibtex</a>]
</span><span class=publications-links>[<a href=https://github.com/Vahe1994/Deep-Vectorization-of-Technical-Drawings>Code</a>]
</span><span class=publications-links>[<a href="https://www.youtube.com/watch?v=lnQNzHJOLvE">Video</a>]
</span><span class=publications-links>[<a href="https://www.dropbox.com/s/ux1lzxw6qbcqi7d/2020-deep-vectorization.pptx.zip?dl=0">Slides</a>]</span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2019-monocular-3d/><img src=https://artonson.github.io/publications/2019-monocular-3d/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2019-monocular-3d/>Monocular 3D Object Detection via Geometric Reasoning on Keypoints</a>
</span><span class=publications-venue>VISAPP
2020</span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Ivan Barabanau, <strong>Alexey Artemov</strong>, Evgeny Burnaev, Vyacheslav Murashkin</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>Monocular 3D object detection is well-known to be a challenging vision task due to the loss of depth information; attempts to recover depth using separate image-only approaches lead to unstable and noisy depth estimates, harming 3D detections. In this paper, we propose a novel keypoint-based approach for 3D object detection and localization from a single RGB image. We build our multi-branch model around 2D keypoint detection in images and complement it with a conceptually simple geometric reasoning method. Our network performs in an end-to-end manner, simultaneously and interdependently estimating 2D characteristics, such as 2D bounding boxes, keypoints, and orientation, along with full 3D pose in the scene. We fuse the outputs of distinct branches, applying a reprojection consistency loss during training. The experimental evaluation on the challenging KITTI dataset benchmark demonstrates that our network achieves state-of-the-art results among other monocular 3D detectors.</span></td></tr><tr><td class=tg-0lax><span class=publications-links>[<a href=https://arxiv.org/pdf/1905.05618>Paper</a>]
</span><span class=publications-links>[<a href=https://artonson.github.io/publications/2019-monocular-3d/cite.bib>Bibtex</a>]
</span><span class=publications-links>[<a href="https://www.dropbox.com/s/yyt3y9uogxajlw0/Artemov_Visapp2020_Mono3d.pptx?dl=0">Slides</a>]</span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2019-abc-dataset/><img src=https://artonson.github.io/publications/2019-abc-dataset/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2019-abc-dataset/>ABC: A Big CAD Model Dataset for Geometric Deep Learning</a>
</span><span class=publications-venue>CVPR
2019</span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, <strong>Alexey Artemov</strong>, Evgeny Burnaev, Marc Alexa, Denis Zorin, Daniele Panozzo</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>We introduce ABC-Dataset, a collection of one million Computer-Aided Design (CAD) models for research of geometric deep learning methods and applications. Each model is a collection of explicitly parametrized curves and surfaces, providing ground truth for differential quantities, patch segmentation, geometric feature detection, and shape reconstruction. Sampling the parametric descriptions of surfaces and curves allows generating data in different formats and resolutions, enabling fair comparisons for a wide range of geometric learning algorithms. As a use case for our dataset, we perform a large-scale benchmark for estimation of surface normals, comparing existing data driven methods and evaluating their performance against both the ground truth and traditional normal estimation methods.</span></td></tr><tr><td class=tg-0lax><span class=publications-links>[<a href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Koch_ABC_A_Big_CAD_Model_Dataset_for_Geometric_Deep_Learning_CVPR_2019_paper.pdf>Paper</a>]
</span><span class=publications-links>[<a href=https://openaccess.thecvf.com/content_CVPR_2019/supplemental/Koch_ABC_A_Big_CVPR_2019_supplemental.pdf>Supplementary</a>]
</span><span class=publications-links>[<a href=https://artonson.github.io/publications/2019-abc-dataset/cite.bib>Bibtex</a>]
</span><span class=publications-links>[<a href=https://deep-geometry.github.io/abc-dataset/>Project page</a>]</span></td></tr><tr class=publications-separator><td></tr><tr><td class=publications-teaser rowspan=5 width=256><a href=https://artonson.github.io/publications/2019-perceptual-deep/><img src=https://artonson.github.io/publications/2019-perceptual-deep/image-thumb.jpg width=512/></a></td><td class=tg-0pky><span class=publications-title><a href=https://artonson.github.io/publications/2019-perceptual-deep/>Perceptual Deep Depth Super-Resolution</a>
</span><span class=publications-venue>ICCV
2019</span></td></tr><tr><td class=tg-0pky><span class=publications-authors>Oleg Voynov, <strong>Alexey Artemov</strong>, Vage Egiazarian, Alexander Notchenko, Gleb Bobrovskikh, Evgeny Burnaev, Denis Zorin</span></td></tr><tr><td class=tg-0pky><span class=publications-abstract>RGBD images, combining high-resolution color and lower-resolution depth from various types of depth sensors, are increasingly common. One can significantly improve the resolution of depth maps by taking advantage of color information; deep learning methods make combining color and depth information particularly easy. However, fusing these two sources of data may lead to a variety of artifacts. If depth maps are used to reconstruct 3D shapes, e.g., for virtual reality applications, the visual quality of upsampled images is particularly important. The main idea of our approach is to measure the quality of depth map upsampling using renderings of resulting 3D surfaces. We demonstrate that a simple visual appearance-based loss, when used with either a trained CNN or simply a deep prior, yields significantly improved 3D shapes, as measured by a number of existing perceptual metrics. We compare this approach with a number of existing optimization and learning-based techniques.</span></td></tr><tr><td class=tg-0lax><span class=publications-links>[<a href=https://openaccess.thecvf.com/content_ICCV_2019/papers/Voynov_Perceptual_Deep_Depth_Super-Resolution_ICCV_2019_paper.pdf>Paper</a>]
</span><span class=publications-links>[<a href=https://openaccess.thecvf.com/content_ICCV_2019/supplemental/Voynov_Perceptual_Deep_Depth_ICCV_2019_supplemental.pdf>Supplementary</a>]
</span><span class=publications-links>[<a href=https://artonson.github.io/publications/2019-perceptual-deep/cite.bib>Bibtex</a>]
</span><span class=publications-links>[<a href=https://github.com/voyleg/perceptual-depth-sr>Code</a>]</span></td></tr><tr class=publications-separator><td></tr></table><footer><hr>© <a href=https://artonson.github.io>Alexey Artemov</a> 2021–2022 | <a href=mailto:a.artemov@skoltech.ru>Email</a> | <a href=https://github.com/artonson>Github</a> | <a href="https://scholar.google.ru/citations?user=5lVsH-IAAAAJ&amp;hl=en">Scholar</a> | <a href=https://www.youtube.com/channel/UCO946RPg2Yp2PXFzon9_73g>YouTube</a> | <a href=https://www.linkedin.com/in/artonson/>LinkedIn</a> | <a href=https://twitter.com/artonson>Twitter</a></footer></body></html>