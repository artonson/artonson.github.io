<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Publications | Alexey Artemov</title>
<link rel=stylesheet href=https://artonson.github.io/css/style.css>
<link rel=stylesheet href=https://artonson.github.io/css/fonts.css>
</head>
<body>
<nav>
<ul class=menu>
<li><a href=https://artonson.github.io/>Home</a></li>
<li><a href=https://artonson.github.io/publications>Publications</a></li>
<li><a href=https://artonson.github.io/teaching>Teaching</a></li>
<li><a href=https://artonson.github.io/students>Students</a></li>
<li><a href=https://artonson.github.io/code_resources>Code & Resources</a></li>
<li><a href=https://artonson.github.io/press>Press</a></li>
<li class=menu-standard><a href=mailto:a.artemov@skoltech.ru>
<img src=https://simpleicons.org/icons/maildotru.svg style=max-width:3%;min-width:20px alt=E-Mail>
</a></li>
<li class=menu-standard><a href=https://github.com/artonson>
<img src=https://simpleicons.org/icons/github.svg style=max-width:3%;min-width:20px alt="Github repo">
</a></li>
<li class=menu-standard><a href="https://scholar.google.ru/citations?user=5lVsH-IAAAAJ&hl=en">
<img src=https://simpleicons.org/icons/googlescholar.svg style=max-width:3%;min-width:20px alt="Google Scholar">
</a></li>
<li class=menu-standard><a href=https://www.youtube.com/channel/UCO946RPg2Yp2PXFzon9_73g>
<img src=https://simpleicons.org/icons/youtube.svg style=max-width:3%;min-width:20px alt=Youtube>
</a></li>
<li class=menu-standard><a href=https://www.linkedin.com/artonson>
<img src=https://simpleicons.org/icons/linkedin.svg style=max-width:3%;min-width:20px alt=LinkedIn>
</a></li>
<li class=menu-standard><a href=https://twitter.com/artonson>
<img src=https://simpleicons.org/icons/twitter.svg style=max-width:3%;min-width:20px alt=Twitter>
</a></li>
</ul>
<hr>
</nav>
<h1>Publications</h1>
<p>This page contains a list of recent scientific publications and preprints,
sorted from oldest to newest.
You might be interested in other webpages that keep track of my publications,
such as <a href="https://scholar.google.com/citations?user=5lVsH-IAAAAJ">Google Scholar page</a>,
<a href=http://arxiv.org/a/artemov_a_2>Arxiv page</a>, or <a href="https://www.scopus.com/authid/detail.uri?authorId=57117507600">Scopus page</a>.</p>
<table class=tg>
<tr>
<td class=publications-teaser rowspan=5 width=256>
<a href=https://artonson.github.io/publications/2019-monocular-3d/><img src=https://artonson.github.io/publications/2019-monocular-3d/image-thumb.png width=512/></a>
</td>
<td class=tg-0pky>
<span class=publications-title>
<a href=https://artonson.github.io/publications/2019-monocular-3d/>Monocular 3D Object Detection via Geometric Reasoning on Keypoints</a>
</span>
<span class=publications-venue>
VISAPP
2020
</span>
</td>
</tr>
<tr>
<td class=tg-0pky>
<span class=publications-authors>
Ivan Barabanau, <strong>Alexey Artemov</strong>, Evgeny Burnaev, Vyacheslav Murashkin
</span>
</td>
</tr>
<tr>
<td class=tg-0pky>
<span class=publications-abstract>
Monocular 3D object detection is well-known to be a challenging vision task due to the loss of depth information; attempts to recover depth using separate image-only approaches lead to unstable and noisy depth estimates, harming 3D detections. In this paper, we propose a novel keypoint-based approach for 3D object detection and localization from a single RGB image. We build our multi-branch model around 2D keypoint detection in images and complement it with a conceptually simple geometric reasoning method. Our network performs in an end-to-end manner, simultaneously and interdependently estimating 2D characteristics, such as 2D bounding boxes, keypoints, and orientation, along with full 3D pose in the scene. We fuse the outputs of distinct branches, applying a reprojection consistency loss during training. The experimental evaluation on the challenging KITTI dataset benchmark demonstrates that our network achieves state-of-the-art results among other monocular 3D detectors.
</span>
</td>
</tr>
<tr>
<td class=tg-0lax>
<span class=publications-links>
[<a href=https://arxiv.org/pdf/1905.05618>Paper</a>]
</span>
<span class=publications-links>
[<a href=https://artonson.github.io/publications/2019-monocular-3d/cite.bib>Bibtex</a>]
</span>
<span class=publications-links>
[<a href="https://www.dropbox.com/s/yyt3y9uogxajlw0/Artemov_Visapp2020_Mono3d.pptx?dl=0">Slides</a>]
</span>
</td>
</tr>
<tr class=publications-separator><td></tr>
<tr>
<td class=publications-teaser rowspan=5 width=256>
<a href=https://artonson.github.io/publications/2019-abc-dataset/><img src=https://artonson.github.io/publications/2019-abc-dataset/image-thumb.png width=512/></a>
</td>
<td class=tg-0pky>
<span class=publications-title>
<a href=https://artonson.github.io/publications/2019-abc-dataset/>ABC: A Big CAD Model Dataset for Geometric Deep Learning</a>
</span>
<span class=publications-venue>
CVPR
2019
</span>
</td>
</tr>
<tr>
<td class=tg-0pky>
<span class=publications-authors>
Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, <strong>Alexey Artemov</strong>, Evgeny Burnaev, Marc Alexa, Denis Zorin, Daniele Panozzo
</span>
</td>
</tr>
<tr>
<td class=tg-0pky>
<span class=publications-abstract>
We introduce ABC-Dataset, a collection of one million Computer-Aided Design (CAD) models for research of geometric deep learning methods and applications. Each model is a collection of explicitly parametrized curves and surfaces, providing ground truth for differential quantities, patch segmentation, geometric feature detection, and shape reconstruction. Sampling the parametric descriptions of surfaces and curves allows generating data in different formats and resolutions, enabling fair comparisons for a wide range of geometric learning algorithms. As a use case for our dataset, we perform a large-scale benchmark for estimation of surface normals, comparing existing data driven methods and evaluating their performance against both the ground truth and traditional normal estimation methods.
</span>
</td>
</tr>
<tr>
<td class=tg-0lax>
<span class=publications-links>
[<a href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Koch_ABC_A_Big_CAD_Model_Dataset_for_Geometric_Deep_Learning_CVPR_2019_paper.pdf>Paper</a>]
</span>
<span class=publications-links>
[<a href=https://openaccess.thecvf.com/content_CVPR_2019/supplemental/Koch_ABC_A_Big_CVPR_2019_supplemental.pdf>Supplementary</a>]
</span>
<span class=publications-links>
[<a href=https://artonson.github.io/publications/2019-abc-dataset/cite.bib>Bibtex</a>]
</span>
<span class=publications-links>
[<a href=https://deep-geometry.github.io/abc-dataset/>Project page</a>]
</span>
</td>
</tr>
<tr class=publications-separator><td></tr>
<tr>
<td class=publications-teaser rowspan=5 width=256>
<a href=https://artonson.github.io/publications/2019-perceptual-deep/><img src=https://artonson.github.io/publications/2019-perceptual-deep/image-thumb.png width=512/></a>
</td>
<td class=tg-0pky>
<span class=publications-title>
<a href=https://artonson.github.io/publications/2019-perceptual-deep/>Perceptual Deep Depth Super-Resolution</a>
</span>
<span class=publications-venue>
ICCV
2019
</span>
</td>
</tr>
<tr>
<td class=tg-0pky>
<span class=publications-authors>
Oleg Voynov, <strong>Alexey Artemov</strong>, Vage Egiazarian, Alexander Notchenko, Gleb Bobrovskikh, Evgeny Burnaev, Denis Zorin
</span>
</td>
</tr>
<tr>
<td class=tg-0pky>
<span class=publications-abstract>
RGBD images, combining high-resolution color and lower-resolution depth from various types of depth sensors, are increasingly common. One can significantly improve the resolution of depth maps by taking advantage of color information; deep learning methods make combining color and depth information particularly easy. However, fusing these two sources of data may lead to a variety of artifacts. If depth maps are used to reconstruct 3D shapes, e.g., for virtual reality applications, the visual quality of upsampled images is particularly important. The main idea of our approach is to measure the quality of depth map upsampling using renderings of resulting 3D surfaces. We demonstrate that a simple visual appearance-based loss, when used with either a trained CNN or simply a deep prior, yields significantly improved 3D shapes, as measured by a number of existing perceptual metrics. We compare this approach with a number of existing optimization and learning-based techniques.
</span>
</td>
</tr>
<tr>
<td class=tg-0lax>
<span class=publications-links>
[<a href=https://openaccess.thecvf.com/content_ICCV_2019/papers/Voynov_Perceptual_Deep_Depth_Super-Resolution_ICCV_2019_paper.pdf>Paper</a>]
</span>
<span class=publications-links>
[<a href=https://openaccess.thecvf.com/content_ICCV_2019/supplemental/Voynov_Perceptual_Deep_Depth_ICCV_2019_supplemental.pdf>Supplementary</a>]
</span>
<span class=publications-links>
[<a href=https://artonson.github.io/publications/2019-perceptual-deep/cite.bib>Bibtex</a>]
</span>
<span class=publications-links>
[<a href=https://github.com/voyleg/perceptual-depth-sr>Code</a>]
</span>
</td>
</tr>
<tr class=publications-separator><td></tr>
</table>
<footer>
<hr>
Â© <a href=https://artonson.github.io>Alexey Artemov</a> 2021 | <a href=mailto:a.artemov@skoltech.ru>Email</a> | <a href=https://github.com/artonson>Github</a> | <a href="https://scholar.google.ru/citations?user=5lVsH-IAAAAJ&hl=en">Scholar</a> | <a href=https://www.youtube.com/channel/UCO946RPg2Yp2PXFzon9_73g>YouTube</a> | <a href=https://www.linkedin.com/artonson>LinkedIn</a> | <a href=https://twitter.com/artonson>Twitter</a>
</footer>
</body>
</html>