<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Publications | Alexey Artemov</title>
<link rel=stylesheet href=https://artonson.github.io/css/style.css>
<link rel=stylesheet href=https://artonson.github.io/css/fonts.css>
</head>
<body>
<nav>
<ul class=menu>
<li><a href=https://artonson.github.io/>Home</a></li>
<li><a href=https://artonson.github.io/publications>Publications</a></li>
<li><a href=https://artonson.github.io/teaching>Teaching</a></li>
<li><a href=https://artonson.github.io/students>Students</a></li>
<li><a href=https://artonson.github.io/code_resources>Code & Resources</a></li>
<li><a href=https://artonson.github.io/press>Press</a></li>
<li class=menu-standard><a href=mailto:a.artemov@skoltech.ru>
<img src=https://simpleicons.org/icons/maildotru.svg style=max-width:3%;min-width:20px alt=E-Mail>
</a></li>
<li class=menu-standard><a href=https://github.com/artonson>
<img src=https://simpleicons.org/icons/github.svg style=max-width:3%;min-width:20px alt="Github repo">
</a></li>
<li class=menu-standard><a href="https://scholar.google.ru/citations?user=5lVsH-IAAAAJ&hl=en">
<img src=https://simpleicons.org/icons/googlescholar.svg style=max-width:3%;min-width:20px alt="Google Scholar">
</a></li>
<li class=menu-standard><a href=https://www.youtube.com/channel/UCO946RPg2Yp2PXFzon9_73g>
<img src=https://simpleicons.org/icons/youtube.svg style=max-width:3%;min-width:20px alt=Youtube>
</a></li>
<li class=menu-standard><a href=https://www.linkedin.com/artonson>
<img src=https://simpleicons.org/icons/linkedin.svg style=max-width:3%;min-width:20px alt=LinkedIn>
</a></li>
<li class=menu-standard><a href=https://twitter.com/artonson>
<img src=https://simpleicons.org/icons/twitter.svg style=max-width:3%;min-width:20px alt=Twitter>
</a></li>
</ul>
<hr>
</nav>
<h1>Publications</h1>
<p>This page contains a list of recent scientific publications and preprints,
sorted from oldest to newest.
You might be interested in other webpages that keep track of my publications,
such as <a href="https://scholar.google.com/citations?user=5lVsH-IAAAAJ">Google Scholar page</a>,
<a href=http://arxiv.org/a/artemov_a_2>Arxiv page</a>, or <a href="https://www.scopus.com/authid/detail.uri?authorId=57117507600">Scopus page</a>.</p>
<table class=tg>
<tr>
<td class=publications-teaser rowspan=5 width=256>
<a href=https://artonson.github.io/publications/2021-towards-part-based/><img src=https://artonson.github.io/publications/2021-towards-part-based/image-thumb.jpg width=512/></a>
</td>
<td class=tg-0pky>
<span class=publications-title>
<a href=https://artonson.github.io/publications/2021-towards-part-based/>Towards Part-Based Understanding of RGB-D Scans</a>
</span>
<span class=publications-venue>
CVPR
2021
</span>
</td>
</tr>
<tr>
<td class=tg-0pky>
<span class=publications-authors>
Alexey Bokhovkin, Vladislav Ishimtsev, Emil Bogomolov, Denis Zorin, <strong>Alexey Artemov</strong>, Evgeny Burnaev, Angela Dai
</span>
</td>
</tr>
<tr>
<td class=tg-0pky>
<span class=publications-abstract>
Recent advances in 3D semantic scene understanding have shown impressive progress in 3D instance segmentation, enabling object-level reasoning about 3D scenes; however, a finer-grained understanding is required to enable interactions with objects and their functional understanding. Thus, we propose the task of part-based scene understanding of real-world 3D environments: from an RGB-D scan of a scene, we detect objects, and for each object predict its decomposition into geometric part masks, which composed together form the complete geometry of the observed object. We leverage an intermediary part graph representation to enable robust completion as well as building of part priors, which we use to construct the final part mask predictions. Our experiments demonstrate that guiding part understanding through part graph to part prior-based predictions significantly outperforms alternative approaches to the task of semantic part completion.
</span>
</td>
</tr>
<tr>
<td class=tg-0lax>
<span class=publications-links>
[<a href=https://openaccess.thecvf.com/content/CVPR2021/papers/Bokhovkin_Towards_Part-Based_Understanding_of_RGB-D_Scans_CVPR_2021_paper.pdf>Paper</a>]
</span>
<span class=publications-links>
[<a href=https://openaccess.thecvf.com/content/CVPR2021/supplemental/Bokhovkin_Towards_Part-Based_Understanding_CVPR_2021_supplemental.pdf>Supplementary</a>]
</span>
<span class=publications-links>
[<a href=https://artonson.github.io/publications/2021-towards-part-based/cite.bib>Bibtex</a>]
</span>
<span class=publications-links>
[<a href=https://github.com/alexeybokhovkin/part-based-scan-understanding>Code</a>]
</span>
<span class=publications-links>
[<a href="https://www.youtube.com/watch?v=iuixmPNs4v4">Video</a>]
</span>
<span class=publications-links>
[<a href="https://www.dropbox.com/s/f4ne7k6uim7vqck/2021-towards-part-based.pptx.zip?dl=0">Slides</a>]
</span>
<span class=publications-links>
[<a href=https://www.3dunderstanding.org/papers/2021/bokhovkin2021towards/>Project page</a>]
</span>
</td>
</tr>
<tr class=publications-separator><td></tr>
<tr>
<td class=publications-teaser rowspan=5 width=256>
<a href=https://artonson.github.io/publications/2021-dense-pose/><img src=https://artonson.github.io/publications/2021-dense-pose/image-thumb.jpg width=512/></a>
</td>
<td class=tg-0pky>
<span class=publications-title>
<a href=https://artonson.github.io/publications/2021-dense-pose/>Making DensePose Fast and Light</a>
</span>
<span class=publications-venue>
WACV
2021
</span>
</td>
</tr>
<tr>
<td class=tg-0pky>
<span class=publications-authors>
Ruslan Rakhimov, Emil Bogomolov, Alexandr Notchenko, Fung Mao, <strong>Alexey Artemov</strong>, Denis Zorin, Evgeny Burnaev
</span>
</td>
</tr>
<tr>
<td class=tg-0pky>
<span class=publications-abstract>
DensePose estimation task is a significant step forward for enhancing user experience computer vision applications ranging from augmented reality to cloth fitting. Existing neural network models capable of solving this task are heavily parameterized and a long way from being transferred to an embedded or mobile device. To enable Dense Pose inference on the end device with current models, one needs to support an expensive server-side infrastructure and have a stable internet connection. To make things worse, mobile and embedded devices do not always have a powerful GPU inside. In this work, we target the problem of redesigning the DensePose R-CNN model&rsquo;s architecture so that the final network retains most of its accuracy but becomes more light-weight and fast. To achieve that, we tested and incorporated many deep learning innovations from recent years, specifically performing an ablation study on 23 efficient backbone architectures, multiple two-stage detection pipeline modifications, and custom model quantization methods. As a result, we achieved 17 times model size reduction and 2 times latency improvement compared to the baseline model.
</span>
</td>
</tr>
<tr>
<td class=tg-0lax>
<span class=publications-links>
[<a href=https://openaccess.thecvf.com/content/WACV2021/papers/Rakhimov_Making_DensePose_Fast_and_Light_WACV_2021_paper.pdf>Paper</a>]
</span>
<span class=publications-links>
[<a href=https://artonson.github.io/publications/2021-dense-pose/cite.bib>Bibtex</a>]
</span>
<span class=publications-links>
[<a href=https://github.com/zetyquickly/DensePoseFnL>Code</a>]
</span>
<span class=publications-links>
[<a href="https://www.dropbox.com/s/89jx45uibpqjx8p/2021-dense-pose.key.zip?dl=0">Slides</a>]
</span>
</td>
</tr>
<tr class=publications-separator><td></tr>
<tr>
<td class=publications-teaser rowspan=5 width=256>
<a href=https://artonson.github.io/publications/2021-unpaired-depth-sr/><img src=https://artonson.github.io/publications/2021-unpaired-depth-sr/image-thumb.jpg width=512/></a>
</td>
<td class=tg-0pky>
<span class=publications-title>
<a href=https://artonson.github.io/publications/2021-unpaired-depth-sr/>Towards Unpaired Depth Enhancement and Super-Resolution in the Wild</a>
</span>
<span class=publications-venue>
arXiv
2021
</span>
</td>
</tr>
<tr>
<td class=tg-0pky>
<span class=publications-authors>
Aleksandr Safin, Maxim Kan, Nikita Drobyshev, Oleg Voynov, <strong>Alexey Artemov</strong>, Alexander Filippov, Denis Zorin, Evgeny Burnaev
</span>
</td>
</tr>
<tr>
<td class=tg-0pky>
<span class=publications-abstract>
Depth maps captured with commodity sensors are often of low quality and resolution; these maps need to be enhanced to be used in many applications. State-of-the-art data-driven methods of depth map super-resolution rely on registered pairs of low- and high-resolution depth maps of the same scenes. Acquisition of real-world paired data requires specialized setups. Another alternative, generating low-resolution maps from high-resolution maps by subsampling, adding noise and other artificial degradation methods, does not fully capture the characteristics of real-world low-resolution images. As a consequence, supervised learning methods trained on such artificial paired data may not perform well on real-world low-resolution inputs. We consider an approach to depth super-resolution based on learning from \emph{unpaired data}. While many techniques for unpaired image-to-image translation have been proposed, most fail to deliver effective hole-filling or reconstruct accurate surfaces using depth maps. We propose an unpaired learning method for depth super-resolution, which is based on a learnable degradation model, enhancement component and surface normal estimates as features to produce more accurate depth maps. We propose a benchmark for unpaired depth SR and demonstrate that our method outperforms existing unpaired methods and performs on par with paired.
</span>
</td>
</tr>
<tr>
<td class=tg-0lax>
<span class=publications-links>
[<a href=https://arxiv.org/pdf/2105.12038>Paper</a>]
</span>
<span class=publications-links>
[<a href=https://artonson.github.io/publications/2021-unpaired-depth-sr/cite.bib>Bibtex</a>]
</span>
</td>
</tr>
<tr class=publications-separator><td></tr>
<tr>
<td class=publications-teaser rowspan=5 width=256>
<a href=https://artonson.github.io/publications/2019-monocular-3d/><img src=https://artonson.github.io/publications/2019-monocular-3d/image-thumb.jpg width=512/></a>
</td>
<td class=tg-0pky>
<span class=publications-title>
<a href=https://artonson.github.io/publications/2019-monocular-3d/>Monocular 3D Object Detection via Geometric Reasoning on Keypoints</a>
</span>
<span class=publications-venue>
VISAPP
2020
</span>
</td>
</tr>
<tr>
<td class=tg-0pky>
<span class=publications-authors>
Ivan Barabanau, <strong>Alexey Artemov</strong>, Evgeny Burnaev, Vyacheslav Murashkin
</span>
</td>
</tr>
<tr>
<td class=tg-0pky>
<span class=publications-abstract>
Monocular 3D object detection is well-known to be a challenging vision task due to the loss of depth information; attempts to recover depth using separate image-only approaches lead to unstable and noisy depth estimates, harming 3D detections. In this paper, we propose a novel keypoint-based approach for 3D object detection and localization from a single RGB image. We build our multi-branch model around 2D keypoint detection in images and complement it with a conceptually simple geometric reasoning method. Our network performs in an end-to-end manner, simultaneously and interdependently estimating 2D characteristics, such as 2D bounding boxes, keypoints, and orientation, along with full 3D pose in the scene. We fuse the outputs of distinct branches, applying a reprojection consistency loss during training. The experimental evaluation on the challenging KITTI dataset benchmark demonstrates that our network achieves state-of-the-art results among other monocular 3D detectors.
</span>
</td>
</tr>
<tr>
<td class=tg-0lax>
<span class=publications-links>
[<a href=https://arxiv.org/pdf/1905.05618>Paper</a>]
</span>
<span class=publications-links>
[<a href=https://artonson.github.io/publications/2019-monocular-3d/cite.bib>Bibtex</a>]
</span>
<span class=publications-links>
[<a href="https://www.dropbox.com/s/yyt3y9uogxajlw0/Artemov_Visapp2020_Mono3d.pptx?dl=0">Slides</a>]
</span>
</td>
</tr>
<tr class=publications-separator><td></tr>
<tr>
<td class=publications-teaser rowspan=5 width=256>
<a href=https://artonson.github.io/publications/2019-abc-dataset/><img src=https://artonson.github.io/publications/2019-abc-dataset/image-thumb.jpg width=512/></a>
</td>
<td class=tg-0pky>
<span class=publications-title>
<a href=https://artonson.github.io/publications/2019-abc-dataset/>ABC: A Big CAD Model Dataset for Geometric Deep Learning</a>
</span>
<span class=publications-venue>
CVPR
2019
</span>
</td>
</tr>
<tr>
<td class=tg-0pky>
<span class=publications-authors>
Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, <strong>Alexey Artemov</strong>, Evgeny Burnaev, Marc Alexa, Denis Zorin, Daniele Panozzo
</span>
</td>
</tr>
<tr>
<td class=tg-0pky>
<span class=publications-abstract>
We introduce ABC-Dataset, a collection of one million Computer-Aided Design (CAD) models for research of geometric deep learning methods and applications. Each model is a collection of explicitly parametrized curves and surfaces, providing ground truth for differential quantities, patch segmentation, geometric feature detection, and shape reconstruction. Sampling the parametric descriptions of surfaces and curves allows generating data in different formats and resolutions, enabling fair comparisons for a wide range of geometric learning algorithms. As a use case for our dataset, we perform a large-scale benchmark for estimation of surface normals, comparing existing data driven methods and evaluating their performance against both the ground truth and traditional normal estimation methods.
</span>
</td>
</tr>
<tr>
<td class=tg-0lax>
<span class=publications-links>
[<a href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Koch_ABC_A_Big_CAD_Model_Dataset_for_Geometric_Deep_Learning_CVPR_2019_paper.pdf>Paper</a>]
</span>
<span class=publications-links>
[<a href=https://openaccess.thecvf.com/content_CVPR_2019/supplemental/Koch_ABC_A_Big_CVPR_2019_supplemental.pdf>Supplementary</a>]
</span>
<span class=publications-links>
[<a href=https://artonson.github.io/publications/2019-abc-dataset/cite.bib>Bibtex</a>]
</span>
<span class=publications-links>
[<a href=https://deep-geometry.github.io/abc-dataset/>Project page</a>]
</span>
</td>
</tr>
<tr class=publications-separator><td></tr>
<tr>
<td class=publications-teaser rowspan=5 width=256>
<a href=https://artonson.github.io/publications/2019-perceptual-deep/><img src=https://artonson.github.io/publications/2019-perceptual-deep/image-thumb.jpg width=512/></a>
</td>
<td class=tg-0pky>
<span class=publications-title>
<a href=https://artonson.github.io/publications/2019-perceptual-deep/>Perceptual Deep Depth Super-Resolution</a>
</span>
<span class=publications-venue>
ICCV
2019
</span>
</td>
</tr>
<tr>
<td class=tg-0pky>
<span class=publications-authors>
Oleg Voynov, <strong>Alexey Artemov</strong>, Vage Egiazarian, Alexander Notchenko, Gleb Bobrovskikh, Evgeny Burnaev, Denis Zorin
</span>
</td>
</tr>
<tr>
<td class=tg-0pky>
<span class=publications-abstract>
RGBD images, combining high-resolution color and lower-resolution depth from various types of depth sensors, are increasingly common. One can significantly improve the resolution of depth maps by taking advantage of color information; deep learning methods make combining color and depth information particularly easy. However, fusing these two sources of data may lead to a variety of artifacts. If depth maps are used to reconstruct 3D shapes, e.g., for virtual reality applications, the visual quality of upsampled images is particularly important. The main idea of our approach is to measure the quality of depth map upsampling using renderings of resulting 3D surfaces. We demonstrate that a simple visual appearance-based loss, when used with either a trained CNN or simply a deep prior, yields significantly improved 3D shapes, as measured by a number of existing perceptual metrics. We compare this approach with a number of existing optimization and learning-based techniques.
</span>
</td>
</tr>
<tr>
<td class=tg-0lax>
<span class=publications-links>
[<a href=https://openaccess.thecvf.com/content_ICCV_2019/papers/Voynov_Perceptual_Deep_Depth_Super-Resolution_ICCV_2019_paper.pdf>Paper</a>]
</span>
<span class=publications-links>
[<a href=https://openaccess.thecvf.com/content_ICCV_2019/supplemental/Voynov_Perceptual_Deep_Depth_ICCV_2019_supplemental.pdf>Supplementary</a>]
</span>
<span class=publications-links>
[<a href=https://artonson.github.io/publications/2019-perceptual-deep/cite.bib>Bibtex</a>]
</span>
<span class=publications-links>
[<a href=https://github.com/voyleg/perceptual-depth-sr>Code</a>]
</span>
</td>
</tr>
<tr class=publications-separator><td></tr>
</table>
<footer>
<hr>
© <a href=https://artonson.github.io>Alexey Artemov</a> 2021 | <a href=mailto:a.artemov@skoltech.ru>Email</a> | <a href=https://github.com/artonson>Github</a> | <a href="https://scholar.google.ru/citations?user=5lVsH-IAAAAJ&hl=en">Scholar</a> | <a href=https://www.youtube.com/channel/UCO946RPg2Yp2PXFzon9_73g>YouTube</a> | <a href=https://www.linkedin.com/artonson>LinkedIn</a> | <a href=https://twitter.com/artonson>Twitter</a>
</footer>
</body>
</html>