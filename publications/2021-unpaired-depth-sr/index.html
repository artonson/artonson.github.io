<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Towards Unpaired Depth Enhancement and Super-Resolution in the Wild | Alexey Artemov</title>
<link rel=stylesheet href=https://artonson.github.io/css/style.css><link rel=stylesheet href=https://artonson.github.io/css/fonts.css></head><body><nav><ul class=menu><li><a href=https://artonson.github.io/>Home</a></li><li><a href=https://artonson.github.io/cv_alexey_artemov.pdf>CV</a></li><li><a href=https://artonson.github.io/publications>Research</a></li><li><a href=https://artonson.github.io/teaching>Education</a></li><li><a href=https://artonson.github.io/art>Art & Heritage</a></li><li class=menu-standard><a href=mailto:artonson@yandex.ru><img src=https://simpleicons.org/icons/maildotru.svg style=max-width:3%;min-width:18px alt=E-Mail></a></li><li class=menu-standard><a href=https://github.com/artonson><img src=https://simpleicons.org/icons/github.svg style=max-width:3%;min-width:18px alt="Github repo"></a></li><li class=menu-standard><a href="https://scholar.google.ru/citations?user=5lVsH-IAAAAJ&hl=en"><img src=https://simpleicons.org/icons/googlescholar.svg style=max-width:3%;min-width:18px alt="Google Scholar"></a></li><li class=menu-standard><a href=https://www.youtube.com/channel/UCO946RPg2Yp2PXFzon9_73g><img src=https://simpleicons.org/icons/youtube.svg style=max-width:3%;min-width:18px alt=Youtube></a></li><li class=menu-standard><a href=https://www.linkedin.com/artonson><img src=https://simpleicons.org/icons/linkedin.svg style=max-width:3%;min-width:18px alt=LinkedIn></a></li><li class=menu-standard><a href=https://twitter.com/artonson><img src=https://simpleicons.org/icons/twitter.svg style=max-width:3%;min-width:18px alt=Twitter></a></li><li class=menu-standard><a href=https://t.me/research_meets_art><img src=https://simpleicons.org/icons/telegram.svg style=max-width:3%;min-width:18px alt="Telegram Channel"></a></li></ul><hr></nav><div class=article-meta><h1>Towards Unpaired Depth Enhancement and Super-Resolution in the Wild</h1></div><div class=single-authors-wrap><table class=single-authors id=single-authors><tr><td>Aleksandr Safin</td><td>Maxim Kan</td><td>Nikita Drobyshev</td><td>Oleg Voynov</td></tr><tr><td>Alexey Artemov</td><td>Alexander Filippov</td><td>Denis Zorin</td><td>Evgeny Burnaev</td></tr><tr></tr></table></div><div class=publication-single-teaser><img src=https://artonson.github.io/publications/2021-unpaired-depth-sr/image-big.jpg width=768/></div><div class=publication-single-venue>arXiv</div><div><h2>Abstract</h2><span class=publications-single-abstract>Depth maps captured with commodity sensors are often of low quality and resolution; these maps need to be enhanced to be used in many applications. State-of-the-art data-driven methods of depth map super-resolution rely on registered pairs of low- and high-resolution depth maps of the same scenes. Acquisition of real-world paired data requires specialized setups. Another alternative, generating low-resolution maps from high-resolution maps by subsampling, adding noise and other artificial degradation methods, does not fully capture the characteristics of real-world low-resolution images. As a consequence, supervised learning methods trained on such artificial paired data may not perform well on real-world low-resolution inputs. We consider an approach to depth super-resolution based on learning from <em>unpaired data</em>. While many techniques for unpaired image-to-image translation have been proposed, most fail to deliver effective hole-filling or reconstruct accurate surfaces using depth maps. We propose an unpaired learning method for depth super-resolution, which is based on a learnable degradation model, enhancement component and surface normal estimates as features to produce more accurate depth maps. We propose a benchmark for unpaired depth SR and demonstrate that our method outperforms existing unpaired methods and performs on par with paired.</span></div><main></main><div><h2>Resources</h2><ul><li>Fulltext PDF: <a href=https://arxiv.org/pdf/2105.12038>Towards Unpaired Depth Enhancement and Super-Resolution in the Wild</a></li><li>Citing:<pre tabindex=0><code>@article{2021-unpaired-depth-sr,
 author = {Safin, Aleksandr and Kan, Maxim and Drobyshev, Nikita and Voynov, Oleg and Artemov, Alexey and Filippov, Alexander and Zorin, Denis and Burnaev, Evgeny},
 journal = {arXiv preprint arXiv:2105.12038},
 title = {Towards Unpaired Depth Enhancement and Super-Resolution in the Wild},
 year = {2021}
}
</code></pre></li></ul></div><footer><hr>© <a href=https://artonson.github.io>Alexey Artemov</a> 2021–2024 | <a href=mailto:artonson@yandex.ru>Email</a> | <a href=https://github.com/artonson>Github</a> | <a href="https://scholar.google.ru/citations?user=5lVsH-IAAAAJ&amp;hl=en">Scholar</a> | <a href=https://www.youtube.com/channel/UCO946RPg2Yp2PXFzon9_73g>YouTube</a> | <a href=https://www.linkedin.com/in/artonson/>LinkedIn</a> | <a href=https://twitter.com/artonson>Twitter</a> | <a href=https://t.me/research_meets_art>Telegram</a></footer></body></html>