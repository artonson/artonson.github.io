<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>SSR-2D: Semantic 3D Scene Reconstruction from 2D Images | Alexey Artemov</title>
<link rel=stylesheet href=https://artonson.github.io/css/style.css><link rel=stylesheet href=https://artonson.github.io/css/fonts.css></head><body><nav><ul class=menu><li><a href=https://artonson.github.io/>Home</a></li><li><a href=https://artonson.github.io/cv_alexey_artemov.pdf>CV</a></li><li><a href=https://artonson.github.io/publications>Research</a></li><li><a href=https://artonson.github.io/teaching>Education</a></li><li><a href=https://artonson.github.io/art>Art & Heritage</a></li><li><a href=https://artonson.github.io/press>Press</a></li><li class=menu-standard><a href=mailto:artonson@yandex.ru><img src=https://simpleicons.org/icons/maildotru.svg style=max-width:3%;min-width:18px alt=E-Mail></a></li><li class=menu-standard><a href=https://github.com/artonson><img src=https://simpleicons.org/icons/github.svg style=max-width:3%;min-width:18px alt="Github repo"></a></li><li class=menu-standard><a href="https://scholar.google.ru/citations?user=5lVsH-IAAAAJ&hl=en"><img src=https://simpleicons.org/icons/googlescholar.svg style=max-width:3%;min-width:18px alt="Google Scholar"></a></li><li class=menu-standard><a href=https://www.youtube.com/channel/UCO946RPg2Yp2PXFzon9_73g><img src=https://simpleicons.org/icons/youtube.svg style=max-width:3%;min-width:18px alt=Youtube></a></li><li class=menu-standard><a href=https://www.linkedin.com/artonson><img src=https://simpleicons.org/icons/linkedin.svg style=max-width:3%;min-width:18px alt=LinkedIn></a></li><li class=menu-standard><a href=https://twitter.com/artonson><img src=https://simpleicons.org/icons/twitter.svg style=max-width:3%;min-width:18px alt=Twitter></a></li><li class=menu-standard><a href=https://t.me/research_meets_art><img src=https://simpleicons.org/icons/telegram.svg style=max-width:3%;min-width:18px alt="Telegram Channel"></a></li></ul><hr></nav><div class=article-meta><h1>SSR-2D: Semantic 3D Scene Reconstruction from 2D Images</h1></div><div class=single-authors-wrap><table class=single-authors id=single-authors><tr><td>Junwen Huang</td><td>Alexey Artemov</td><td>Yujin Chen</td><td>Shuaifeng Zhi</td></tr><tr><td>Kai Xu</td><td>Matthias Nießner</td></tr></table></div><div class=publication-single-teaser><img src=https://artonson.github.io/publications/2024-ssr-2d/image-big.jpg width=768/></div><div class=publication-single-venue>ArXiv</div><div><h2>Abstract</h2><span class=publications-single-abstract>Most deep learning approaches to comprehensive semantic modeling of 3D indoor spaces require costly dense annotations in the 3D domain. In this work, we explore a central 3D scene modeling task, namely, semantic scene reconstruction without using any 3D annotations. The key idea of our approach is to design a trainable model that employs both incomplete 3D reconstructions and their corresponding source RGB-D images, fusing cross-domain features into volumetric embeddings to predict complete 3D geometry, color, and semantics with only 2D labeling which can be either manual or machine-generated. Our key technical innovation is to leverage differentiable rendering of color and semantics to bridge 2D observations and unknown 3D space, using the observed RGB images and 2D semantics as supervision, respectively. We additionally develop a learning pipeline and corresponding method to enable learning from imperfect predicted 2D labels, which could be additionally acquired by synthesizing in an augmented set of virtual training views complementing the original real captures, enabling more efficient self-supervision loop for semantics. In this work, we propose an end-to-end trainable solution jointly addressing geometry completion, colorization, and semantic mapping from limited RGB-D images, without relying on any 3D ground-truth information. Our method achieves state-of-the-art performance of semantic scene reconstruction on two large-scale benchmark datasets MatterPort3D and ScanNet, surpasses baselines even with costly 3D annotations. To our knowledge, our method is also the first 2D-driven method addressing completion and semantic segmentation of real-world 3D scans.</span></div><main></main><div><h2>Resources</h2><ul><li>Fulltext PDF: <a href=https://arxiv.org/pdf/2302.03640>SSR-2D: Semantic 3D Scene Reconstruction from 2D Images</a></li><li>Citing:<pre tabindex=0><code>@article{2024-ssr-2d,
  title={SSR-2D: Semantic 3D Scene Reconstruction from 2D Images},
  author={Huang, Junwen and Artemov, Alexey and Chen, Yujin and Zhi, Shuaifeng and Xu, Kai and Nie{\ss}ner, Matthias},
  journal={arXiv preprint arXiv:2302.03640},
  year={2023}
}
</code></pre></li></ul></div><footer><hr>© <a href=https://artonson.github.io>Alexey Artemov</a> 2021–2024 | <a href=mailto:artonson@yandex.ru>Email</a> | <a href=https://github.com/artonson>Github</a> | <a href="https://scholar.google.ru/citations?user=5lVsH-IAAAAJ&amp;hl=en">Scholar</a> | <a href=https://www.youtube.com/channel/UCO946RPg2Yp2PXFzon9_73g>YouTube</a> | <a href=https://www.linkedin.com/in/artonson/>LinkedIn</a> | <a href=https://twitter.com/artonson>Twitter</a> | <a href=https://t.me/research_meets_art>Telegram</a></footer></body></html>