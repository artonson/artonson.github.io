<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Perceptual Deep Depth Super-Resolution | Alexey Artemov</title><link rel=stylesheet href=https://artonson.github.io/css/style.css><link rel=stylesheet href=https://artonson.github.io/css/fonts.css></head><body><nav><ul class=menu><li><a href=https://artonson.github.io/>Home</a></li><li><a href=https://artonson.github.io/cv_alexey_artemov.pdf>CV</a></li><li><a href=https://artonson.github.io/publications>Publications</a></li><li><a href=https://artonson.github.io/teaching>Teaching</a></li><li><a href=https://artonson.github.io/students>Students</a></li><li><a href=https://artonson.github.io/code_resources>Code & Resources</a></li><li><a href=https://artonson.github.io/press>Press</a></li><li class=menu-standard><a href=mailto:a.artemov@skoltech.ru><img src=https://simpleicons.org/icons/maildotru.svg style=max-width:3%;min-width:20px alt=E-Mail></a></li><li class=menu-standard><a href=https://github.com/artonson><img src=https://simpleicons.org/icons/github.svg style=max-width:3%;min-width:20px alt="Github repo"></a></li><li class=menu-standard><a href="https://scholar.google.ru/citations?user=5lVsH-IAAAAJ&hl=en"><img src=https://simpleicons.org/icons/googlescholar.svg style=max-width:3%;min-width:20px alt="Google Scholar"></a></li><li class=menu-standard><a href=https://www.youtube.com/channel/UCO946RPg2Yp2PXFzon9_73g><img src=https://simpleicons.org/icons/youtube.svg style=max-width:3%;min-width:20px alt=Youtube></a></li><li class=menu-standard><a href=https://www.linkedin.com/artonson><img src=https://simpleicons.org/icons/linkedin.svg style=max-width:3%;min-width:20px alt=LinkedIn></a></li><li class=menu-standard><a href=https://twitter.com/artonson><img src=https://simpleicons.org/icons/twitter.svg style=max-width:3%;min-width:20px alt=Twitter></a></li></ul><hr></nav><div class=article-meta><h1>Perceptual Deep Depth Super-Resolution</h1></div><div class=single-authors-wrap><table class=single-authors id=single-authors><tr><td>Oleg Voynov</td><td>Alexey Artemov</td><td>Vage Egiazarian</td><td>Alexander Notchenko</td><td>Gleb Bobrovskikh</td></tr><tr><td>Evgeny Burnaev</td><td>Denis Zorin</td></tr></table></div><div class=publication-single-teaser><img src=https://artonson.github.io/publications/2019-perceptual-deep/image-big.jpg width=768/></div><div class=publication-single-venue>ICCV
2019</div><div><h2>Abstract</h2><span class=publications-single-abstract>RGBD images, combining high-resolution color and lower-resolution depth from various types of depth sensors, are increasingly common. One can significantly improve the resolution of depth maps by taking advantage of color information; deep learning methods make combining color and depth information particularly easy. However, fusing these two sources of data may lead to a variety of artifacts. If depth maps are used to reconstruct 3D shapes, e.g., for virtual reality applications, the visual quality of upsampled images is particularly important. The main idea of our approach is to measure the quality of depth map upsampling using renderings of resulting 3D surfaces. We demonstrate that a simple visual appearance-based loss, when used with either a trained CNN or simply a deep prior, yields significantly improved 3D shapes, as measured by a number of existing perceptual metrics. We compare this approach with a number of existing optimization and learning-based techniques.</span></div><main></main><div><h2>Resources</h2><ul><li>Fulltext PDF: <a href=https://openaccess.thecvf.com/content_ICCV_2019/papers/Voynov_Perceptual_Deep_Depth_Super-Resolution_ICCV_2019_paper.pdf>Perceptual Deep Depth Super-Resolution</a></li><li>Supplementary PDF: <a href=https://openaccess.thecvf.com/content_ICCV_2019/supplemental/Voynov_Perceptual_Deep_Depth_ICCV_2019_supplemental.pdf>Perceptual Deep Depth Super-Resolution Supplementary Materials</a></li><li>Source code: <a href=https://github.com/voyleg/perceptual-depth-sr>https://github.com/voyleg/perceptual-depth-sr</a></li><li>Citing:<pre tabindex=0><code>@inproceedings{2019-perceptual-deep,
 author = {Voynov, Oleg and Artemov, Alexey and Egiazarian, Vage and Notchenko, Alexander and Bobrovskikh, Gleb and Burnaev, Evgeny and Zorin, Denis},
 booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
 pages = {5653--5663},
 title = {Perceptual deep depth super-resolution},
 year = {2019}
}
</code></pre></li></ul></div><footer><hr>© <a href=https://artonson.github.io>Alexey Artemov</a> 2021–2022 | <a href=mailto:a.artemov@skoltech.ru>Email</a> | <a href=https://github.com/artonson>Github</a> | <a href="https://scholar.google.ru/citations?user=5lVsH-IAAAAJ&hl=en">Scholar</a> | <a href=https://www.youtube.com/channel/UCO946RPg2Yp2PXFzon9_73g>YouTube</a> | <a href=https://www.linkedin.com/in/artonson/>LinkedIn</a> | <a href=https://twitter.com/artonson>Twitter</a></footer></body></html>